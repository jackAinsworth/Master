nohup: ignoring input
2025-04-09 01:00:17.285333: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-09 01:00:17.333735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-09 01:00:20.736453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46551 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:3d:00.0, compute capability: 8.9
2025-04-09 01:00:20.738092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46551 MB memory:  -> device: 1, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:61:00.0, compute capability: 8.9
2025-04-09 01:00:20.739521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22808 MB memory:  -> device: 2, name: NVIDIA TITAN RTX, pci bus id: 0000:60:00.0, compute capability: 7.5
2025-04-09 01:00:20.740881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22808 MB memory:  -> device: 3, name: NVIDIA TITAN RTX, pci bus id: 0000:64:00.0, compute capability: 7.5
2025-04-09 01:00:20.742229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 9616 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5
Number of devices: 2
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 100, 100, 3)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 100, 100, 64)         4864      ['input_1[0][0]']             
                                                                                                  
 conv2d_2 (Conv2D)           (None, 100, 100, 256)        16640     ['conv2d[0][0]']              
                                                                                                  
 conv2d_4 (Conv2D)           (None, 100, 100, 256)        16640     ['conv2d[0][0]']              
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 100, 100, 64)         0         ['conv2d[0][0]']              
 D)                                                                                               
                                                                                                  
 conv2d_1 (Conv2D)           (None, 100, 100, 256)        16640     ['conv2d[0][0]']              
                                                                                                  
 conv2d_3 (Conv2D)           (None, 100, 100, 256)        590080    ['conv2d_2[0][0]']            
                                                                                                  
 conv2d_5 (Conv2D)           (None, 100, 100, 256)        1638656   ['conv2d_4[0][0]']            
                                                                                                  
 conv2d_6 (Conv2D)           (None, 100, 100, 256)        16640     ['max_pooling2d[0][0]']       
                                                                                                  
 concatenate (Concatenate)   (None, 100, 100, 1024)       0         ['conv2d_1[0][0]',            
                                                                     'conv2d_3[0][0]',            
                                                                     'conv2d_5[0][0]',            
                                                                     'conv2d_6[0][0]']            
                                                                                                  
 conv2d_7 (Conv2D)           (None, 100, 100, 128)        1179776   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization (Batch  (None, 100, 100, 128)        512       ['conv2d_7[0][0]']            
 Normalization)                                                                                   
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 34, 34, 128)          0         ['batch_normalization[0][0]'] 
 g2D)                                                                                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 34, 34, 256)          33024     ['max_pooling2d_1[0][0]']     
                                                                                                  
 conv2d_11 (Conv2D)          (None, 34, 34, 256)          33024     ['max_pooling2d_1[0][0]']     
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 34, 34, 128)          0         ['max_pooling2d_1[0][0]']     
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 34, 34, 256)          33024     ['max_pooling2d_1[0][0]']     
                                                                                                  
 conv2d_10 (Conv2D)          (None, 34, 34, 256)          590080    ['conv2d_9[0][0]']            
                                                                                                  
 conv2d_12 (Conv2D)          (None, 34, 34, 256)          1638656   ['conv2d_11[0][0]']           
                                                                                                  
 conv2d_13 (Conv2D)          (None, 34, 34, 256)          33024     ['max_pooling2d_2[0][0]']     
                                                                                                  
 concatenate_1 (Concatenate  (None, 34, 34, 1024)         0         ['conv2d_8[0][0]',            
 )                                                                   'conv2d_10[0][0]',           
                                                                     'conv2d_12[0][0]',           
                                                                     'conv2d_13[0][0]']           
                                                                                                  
 conv2d_14 (Conv2D)          (None, 34, 34, 512)          4719104   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_1 (Bat  (None, 34, 34, 512)          2048      ['conv2d_14[0][0]']           
 chNormalization)                                                                                 
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 12, 12, 512)          0         ['batch_normalization_1[0][0]'
 g2D)                                                               ]                             
                                                                                                  
 conv2d_16 (Conv2D)          (None, 12, 12, 256)          131328    ['max_pooling2d_3[0][0]']     
                                                                                                  
 conv2d_18 (Conv2D)          (None, 12, 12, 256)          131328    ['max_pooling2d_3[0][0]']     
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 12, 12, 512)          0         ['max_pooling2d_3[0][0]']     
 g2D)                                                                                             
                                                                                                  
 conv2d_15 (Conv2D)          (None, 12, 12, 256)          131328    ['max_pooling2d_3[0][0]']     
                                                                                                  
 conv2d_17 (Conv2D)          (None, 12, 12, 256)          590080    ['conv2d_16[0][0]']           
                                                                                                  
 conv2d_19 (Conv2D)          (None, 12, 12, 256)          1638656   ['conv2d_18[0][0]']           
                                                                                                  
 conv2d_20 (Conv2D)          (None, 12, 12, 256)          131328    ['max_pooling2d_4[0][0]']     
                                                                                                  
 concatenate_2 (Concatenate  (None, 12, 12, 1024)         0         ['conv2d_15[0][0]',           
 )                                                                   'conv2d_17[0][0]',           
                                                                     'conv2d_19[0][0]',           
                                                                     'conv2d_20[0][0]']           
                                                                                                  
 conv2d_21 (Conv2D)          (None, 12, 12, 256)          2359552   ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 12, 12, 256)          1024      ['conv2d_21[0][0]']           
 chNormalization)                                                                                 
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 4, 4, 256)            0         ['batch_normalization_2[0][0]'
 g2D)                                                               ]                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 4, 4, 256)            65792     ['max_pooling2d_5[0][0]']     
                                                                                                  
 conv2d_25 (Conv2D)          (None, 4, 4, 256)            65792     ['max_pooling2d_5[0][0]']     
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 4, 4, 256)            0         ['max_pooling2d_5[0][0]']     
 g2D)                                                                                             
                                                                                                  
 conv2d_22 (Conv2D)          (None, 4, 4, 256)            65792     ['max_pooling2d_5[0][0]']     
                                                                                                  
 conv2d_24 (Conv2D)          (None, 4, 4, 256)            590080    ['conv2d_23[0][0]']           
                                                                                                  
 conv2d_26 (Conv2D)          (None, 4, 4, 256)            1638656   ['conv2d_25[0][0]']           
                                                                                                  
 conv2d_27 (Conv2D)          (None, 4, 4, 256)            65792     ['max_pooling2d_6[0][0]']     
                                                                                                  
 concatenate_3 (Concatenate  (None, 4, 4, 1024)           0         ['conv2d_22[0][0]',           
 )                                                                   'conv2d_24[0][0]',           
                                                                     'conv2d_26[0][0]',           
                                                                     'conv2d_27[0][0]']           
                                                                                                  
 conv2d_28 (Conv2D)          (None, 4, 4, 512)            4719104   ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_3 (Bat  (None, 4, 4, 512)            2048      ['conv2d_28[0][0]']           
 chNormalization)                                                                                 
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 2, 2, 512)            0         ['batch_normalization_3[0][0]'
 g2D)                                                               ]                             
                                                                                                  
 flatten (Flatten)           (None, 2048)                 0         ['max_pooling2d_7[0][0]']     
                                                                                                  
 dense (Dense)               (None, 192)                  393408    ['flatten[0][0]']             
                                                                                                  
 reshape (Reshape)           (None, 8, 8, 3)              0         ['dense[0][0]']               
                                                                                                  
==================================================================================================
Total params: 23283520 (88.82 MB)
Trainable params: 23280704 (88.81 MB)
Non-trainable params: 2816 (11.00 KB)
__________________________________________________________________________________________________
Training set: (28000, 100, 100, 3) (28000, 8, 8, 3)
Validation set: (6000, 100, 100, 3) (6000, 8, 8, 3)
Test set: (6000, 100, 100, 3) (6000, 8, 8, 3)
Epoch 1/200
2025-04-09 01:01:26.437658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
2025-04-09 01:01:26.453720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
2025-04-09 01:01:35.363369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2025-04-09 01:01:42.014298: W tensorflow/core/kernels/gpu_utils.cc:50] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
2025-04-09 01:01:58.492922: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd083bce0f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2025-04-09 01:01:58.492988: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX 6000 Ada Generation, Compute Capability 8.9
2025-04-09 01:01:58.492997: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA RTX 6000 Ada Generation, Compute Capability 8.9
2025-04-09 01:01:58.493003: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA TITAN RTX, Compute Capability 7.5
2025-04-09 01:01:58.493009: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA TITAN RTX, Compute Capability 7.5
2025-04-09 01:01:58.493015: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
2025-04-09 01:01:58.537919: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2025-04-09 01:01:58.891604: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
73/73 - 184s - loss: 413.7582 - val_loss: 22318.2969 - 184s/epoch - 3s/step
Epoch 2/200
73/73 - 99s - loss: 299.7457 - val_loss: 1272.5250 - 99s/epoch - 1s/step
Epoch 3/200
73/73 - 99s - loss: 260.9725 - val_loss: 1225.3774 - 99s/epoch - 1s/step
Epoch 4/200
73/73 - 98s - loss: 214.9509 - val_loss: 297.7013 - 98s/epoch - 1s/step
Epoch 5/200
73/73 - 99s - loss: 155.5468 - val_loss: 398.1877 - 99s/epoch - 1s/step
Epoch 6/200
73/73 - 99s - loss: 98.8806 - val_loss: 256.1796 - 99s/epoch - 1s/step
Epoch 7/200
73/73 - 100s - loss: 78.7744 - val_loss: 279.6280 - 100s/epoch - 1s/step
Epoch 8/200
73/73 - 100s - loss: 54.3800 - val_loss: 362.5369 - 100s/epoch - 1s/step
Epoch 9/200
73/73 - 99s - loss: 46.2389 - val_loss: 126.2091 - 99s/epoch - 1s/step
Epoch 10/200
73/73 - 98s - loss: 45.1791 - val_loss: 89.5469 - 98s/epoch - 1s/step
Epoch 11/200
73/73 - 99s - loss: 43.8610 - val_loss: 671.1617 - 99s/epoch - 1s/step
Epoch 12/200
73/73 - 103s - loss: 38.7387 - val_loss: 1414.5254 - 103s/epoch - 1s/step
Epoch 13/200
73/73 - 102s - loss: 32.4206 - val_loss: 75.1533 - 102s/epoch - 1s/step
Epoch 14/200
73/73 - 102s - loss: 27.5772 - val_loss: 187.0373 - 102s/epoch - 1s/step
Epoch 15/200
73/73 - 101s - loss: 26.0451 - val_loss: 201.4322 - 101s/epoch - 1s/step
Epoch 16/200
73/73 - 100s - loss: 26.4909 - val_loss: 256.9721 - 100s/epoch - 1s/step
Epoch 17/200
73/73 - 101s - loss: 30.8556 - val_loss: 195.0298 - 101s/epoch - 1s/step
Epoch 18/200
73/73 - 101s - loss: 21.2299 - val_loss: 40.0168 - 101s/epoch - 1s/step
Epoch 19/200
73/73 - 101s - loss: 20.3927 - val_loss: 72.8970 - 101s/epoch - 1s/step
Epoch 20/200
73/73 - 103s - loss: 17.4780 - val_loss: 43.9795 - 103s/epoch - 1s/step
Epoch 21/200
73/73 - 102s - loss: 22.6654 - val_loss: 198.5428 - 102s/epoch - 1s/step
Epoch 22/200
73/73 - 101s - loss: 43.9691 - val_loss: 3576.2280 - 101s/epoch - 1s/step
Epoch 23/200
73/73 - 103s - loss: 28.9828 - val_loss: 531.9894 - 103s/epoch - 1s/step
Epoch 24/200
73/73 - 101s - loss: 14.8960 - val_loss: 192.6625 - 101s/epoch - 1s/step
Epoch 25/200
73/73 - 100s - loss: 12.8486 - val_loss: 26.7473 - 100s/epoch - 1s/step
Epoch 26/200
73/73 - 102s - loss: 13.8526 - val_loss: 492.8207 - 102s/epoch - 1s/step
Epoch 27/200
73/73 - 100s - loss: 14.7206 - val_loss: 199.8090 - 100s/epoch - 1s/step
Epoch 28/200
73/73 - 101s - loss: 10.6669 - val_loss: 65.1445 - 101s/epoch - 1s/step
Epoch 29/200
73/73 - 100s - loss: 10.0636 - val_loss: 86.3830 - 100s/epoch - 1s/step
Epoch 30/200
73/73 - 103s - loss: 10.1009 - val_loss: 350.9332 - 103s/epoch - 1s/step
Epoch 31/200
73/73 - 101s - loss: 10.2863 - val_loss: 258.3849 - 101s/epoch - 1s/step
Epoch 32/200
73/73 - 103s - loss: 9.7789 - val_loss: 128.6393 - 103s/epoch - 1s/step
Epoch 33/200
73/73 - 101s - loss: 8.5177 - val_loss: 126.2581 - 101s/epoch - 1s/step
Epoch 34/200
73/73 - 101s - loss: 8.0824 - val_loss: 249.5822 - 101s/epoch - 1s/step
Epoch 35/200
73/73 - 100s - loss: 23.6739 - val_loss: 239.0880 - 100s/epoch - 1s/step
Epoch 36/200
73/73 - 104s - loss: 12.2198 - val_loss: 190.1815 - 104s/epoch - 1s/step
Epoch 37/200
73/73 - 101s - loss: 7.7972 - val_loss: 45.9984 - 101s/epoch - 1s/step
Epoch 38/200
73/73 - 102s - loss: 7.3132 - val_loss: 32.1240 - 102s/epoch - 1s/step
Epoch 39/200
73/73 - 100s - loss: 6.3864 - val_loss: 35.8390 - 100s/epoch - 1s/step
Epoch 40/200
73/73 - 101s - loss: 7.8584 - val_loss: 40.8878 - 101s/epoch - 1s/step
Epoch 41/200
73/73 - 102s - loss: 7.3783 - val_loss: 84.2644 - 102s/epoch - 1s/step
Epoch 42/200
73/73 - 101s - loss: 6.1806 - val_loss: 53.8012 - 101s/epoch - 1s/step
Epoch 43/200
73/73 - 99s - loss: 5.7320 - val_loss: 76.9395 - 99s/epoch - 1s/step
Epoch 44/200
73/73 - 102s - loss: 5.5879 - val_loss: 57.0240 - 102s/epoch - 1s/step
Epoch 45/200
73/73 - 100s - loss: 6.1905 - val_loss: 185.6065 - 100s/epoch - 1s/step
Epoch 46/200
73/73 - 101s - loss: 7.1270 - val_loss: 341.3697 - 101s/epoch - 1s/step
Epoch 47/200
73/73 - 102s - loss: 21.5042 - val_loss: 1363.2430 - 102s/epoch - 1s/step
Epoch 48/200
73/73 - 100s - loss: 17.3829 - val_loss: 508.0085 - 100s/epoch - 1s/step
Epoch 49/200
73/73 - 96s - loss: 7.5749 - val_loss: 42.0996 - 96s/epoch - 1s/step
Epoch 50/200
73/73 - 98s - loss: 9.0847 - val_loss: 57.0940 - 98s/epoch - 1s/step
Epoch 51/200
73/73 - 96s - loss: 5.8284 - val_loss: 29.0930 - 96s/epoch - 1s/step
Epoch 52/200
73/73 - 98s - loss: 4.8125 - val_loss: 20.7726 - 98s/epoch - 1s/step
Epoch 53/200
73/73 - 97s - loss: 4.4823 - val_loss: 26.1571 - 97s/epoch - 1s/step
Epoch 54/200
73/73 - 97s - loss: 4.4536 - val_loss: 22.1378 - 97s/epoch - 1s/step
Epoch 55/200
73/73 - 96s - loss: 4.0679 - val_loss: 11.0145 - 96s/epoch - 1s/step
Epoch 56/200
73/73 - 97s - loss: 3.8211 - val_loss: 8.2855 - 97s/epoch - 1s/step
Epoch 57/200
73/73 - 97s - loss: 5.1713 - val_loss: 210.0926 - 97s/epoch - 1s/step
Epoch 58/200
73/73 - 97s - loss: 5.9010 - val_loss: 391.7057 - 97s/epoch - 1s/step
Epoch 59/200
73/73 - 97s - loss: 4.4513 - val_loss: 187.8395 - 97s/epoch - 1s/step
Epoch 60/200
73/73 - 98s - loss: 3.6762 - val_loss: 7.2910 - 98s/epoch - 1s/step
Epoch 61/200
73/73 - 98s - loss: 4.0969 - val_loss: 80.8501 - 98s/epoch - 1s/step
Epoch 62/200
73/73 - 96s - loss: 4.9399 - val_loss: 51.9587 - 96s/epoch - 1s/step
Epoch 63/200
73/73 - 97s - loss: 3.8531 - val_loss: 57.5944 - 97s/epoch - 1s/step
Epoch 64/200
73/73 - 97s - loss: 4.2008 - val_loss: 36.2735 - 97s/epoch - 1s/step
Epoch 65/200
73/73 - 99s - loss: 3.6571 - val_loss: 23.0511 - 99s/epoch - 1s/step
Epoch 66/200
73/73 - 98s - loss: 3.5703 - val_loss: 105.5400 - 98s/epoch - 1s/step
Epoch 67/200
73/73 - 95s - loss: 3.1205 - val_loss: 20.6448 - 95s/epoch - 1s/step
Epoch 68/200
73/73 - 98s - loss: 3.0706 - val_loss: 8.6879 - 98s/epoch - 1s/step
Epoch 69/200
73/73 - 97s - loss: 2.7423 - val_loss: 10.3074 - 97s/epoch - 1s/step
Epoch 70/200
73/73 - 96s - loss: 2.5918 - val_loss: 9.2783 - 96s/epoch - 1s/step
Epoch 71/200
73/73 - 98s - loss: 2.6008 - val_loss: 14.1790 - 98s/epoch - 1s/step
Epoch 72/200
73/73 - 96s - loss: 2.5589 - val_loss: 16.3011 - 96s/epoch - 1s/step
Epoch 73/200
73/73 - 98s - loss: 6.2502 - val_loss: 411.3368 - 98s/epoch - 1s/step
Epoch 74/200
73/73 - 98s - loss: 21.7465 - val_loss: 840.4468 - 98s/epoch - 1s/step
Epoch 75/200
73/73 - 96s - loss: 5.4264 - val_loss: 213.3601 - 96s/epoch - 1s/step
Epoch 76/200
73/73 - 97s - loss: 4.5811 - val_loss: 72.4796 - 97s/epoch - 1s/step
Epoch 77/200
73/73 - 97s - loss: 4.1821 - val_loss: 108.9194 - 97s/epoch - 1s/step
Epoch 78/200
73/73 - 97s - loss: 3.3919 - val_loss: 38.0515 - 97s/epoch - 1s/step
Epoch 79/200
73/73 - 96s - loss: 2.8232 - val_loss: 52.5813 - 96s/epoch - 1s/step
Epoch 80/200
73/73 - 96s - loss: 2.4660 - val_loss: 16.3547 - 96s/epoch - 1s/step
Epoch 81/200
73/73 - 98s - loss: 2.2460 - val_loss: 6.8228 - 98s/epoch - 1s/step
Epoch 82/200
73/73 - 97s - loss: 2.2199 - val_loss: 10.5978 - 97s/epoch - 1s/step
Epoch 83/200
73/73 - 98s - loss: 2.2308 - val_loss: 8.0035 - 98s/epoch - 1s/step
Epoch 84/200
73/73 - 97s - loss: 2.0303 - val_loss: 5.1565 - 97s/epoch - 1s/step
Epoch 85/200
73/73 - 98s - loss: 1.8714 - val_loss: 3.5554 - 98s/epoch - 1s/step
Epoch 86/200
73/73 - 97s - loss: 1.8696 - val_loss: 4.4457 - 97s/epoch - 1s/step
Epoch 87/200
73/73 - 97s - loss: 1.8485 - val_loss: 3.7761 - 97s/epoch - 1s/step
Epoch 88/200
73/73 - 96s - loss: 1.7813 - val_loss: 4.8219 - 96s/epoch - 1s/step
Epoch 89/200
73/73 - 96s - loss: 1.8360 - val_loss: 3.9128 - 96s/epoch - 1s/step
Epoch 90/200
73/73 - 96s - loss: 1.7831 - val_loss: 5.0607 - 96s/epoch - 1s/step
Epoch 91/200
73/73 - 96s - loss: 1.8211 - val_loss: 7.1153 - 96s/epoch - 1s/step
Epoch 92/200
73/73 - 97s - loss: 1.7921 - val_loss: 8.4961 - 97s/epoch - 1s/step
Epoch 93/200
73/73 - 96s - loss: 1.7187 - val_loss: 3.9107 - 96s/epoch - 1s/step
Epoch 94/200
73/73 - 96s - loss: 1.5988 - val_loss: 4.1423 - 96s/epoch - 1s/step
Epoch 95/200
73/73 - 97s - loss: 1.5860 - val_loss: 5.7509 - 97s/epoch - 1s/step
Epoch 96/200
73/73 - 97s - loss: 1.6043 - val_loss: 5.2378 - 97s/epoch - 1s/step
Epoch 97/200
73/73 - 97s - loss: 1.8480 - val_loss: 8.6150 - 97s/epoch - 1s/step
Epoch 98/200
73/73 - 96s - loss: 14.6789 - val_loss: 6996.6147 - 96s/epoch - 1s/step
Epoch 99/200
73/73 - 96s - loss: 29.4418 - val_loss: 270.4351 - 96s/epoch - 1s/step
Epoch 100/200
73/73 - 97s - loss: 8.7428 - val_loss: 318.8139 - 97s/epoch - 1s/step
Epoch 101/200
73/73 - 96s - loss: 4.6090 - val_loss: 132.9722 - 96s/epoch - 1s/step
Epoch 102/200
73/73 - 97s - loss: 3.4575 - val_loss: 30.8503 - 97s/epoch - 1s/step
Epoch 103/200
73/73 - 96s - loss: 2.8202 - val_loss: 15.7368 - 96s/epoch - 1s/step
Epoch 104/200
73/73 - 95s - loss: 2.3603 - val_loss: 13.1807 - 95s/epoch - 1s/step
Epoch 105/200
73/73 - 95s - loss: 2.0304 - val_loss: 8.3194 - 95s/epoch - 1s/step
Epoch 106/200
73/73 - 95s - loss: 2.3867 - val_loss: 104.0368 - 95s/epoch - 1s/step
Epoch 107/200
73/73 - 95s - loss: 2.5891 - val_loss: 76.9849 - 95s/epoch - 1s/step
Epoch 108/200
73/73 - 96s - loss: 1.9254 - val_loss: 5.9849 - 96s/epoch - 1s/step
Epoch 109/200
73/73 - 95s - loss: 1.6943 - val_loss: 4.5088 - 95s/epoch - 1s/step
Epoch 110/200
73/73 - 95s - loss: 1.6226 - val_loss: 3.1044 - 95s/epoch - 1s/step
Epoch 111/200
73/73 - 94s - loss: 1.7395 - val_loss: 6.2875 - 94s/epoch - 1s/step
Epoch 112/200
73/73 - 93s - loss: 1.5476 - val_loss: 3.1868 - 93s/epoch - 1s/step
Epoch 113/200
73/73 - 95s - loss: 1.5211 - val_loss: 3.5999 - 95s/epoch - 1s/step
Epoch 114/200
73/73 - 96s - loss: 1.4944 - val_loss: 2.9005 - 96s/epoch - 1s/step
Epoch 115/200
73/73 - 96s - loss: 1.4336 - val_loss: 3.5336 - 96s/epoch - 1s/step
Epoch 116/200
73/73 - 95s - loss: 1.3932 - val_loss: 3.0907 - 95s/epoch - 1s/step
Epoch 117/200
73/73 - 96s - loss: 1.3979 - val_loss: 3.1983 - 96s/epoch - 1s/step
Epoch 118/200
73/73 - 97s - loss: 1.4365 - val_loss: 5.4555 - 97s/epoch - 1s/step
Epoch 119/200
73/73 - 95s - loss: 1.3847 - val_loss: 7.4133 - 95s/epoch - 1s/step
Epoch 120/200
73/73 - 97s - loss: 1.3820 - val_loss: 3.4681 - 97s/epoch - 1s/step
Epoch 121/200
73/73 - 95s - loss: 1.3402 - val_loss: 2.8618 - 95s/epoch - 1s/step
Epoch 122/200
73/73 - 95s - loss: 1.3157 - val_loss: 3.9313 - 95s/epoch - 1s/step
Epoch 123/200
73/73 - 96s - loss: 1.3019 - val_loss: 3.4283 - 96s/epoch - 1s/step
Epoch 124/200
73/73 - 94s - loss: 1.2304 - val_loss: 3.3657 - 94s/epoch - 1s/step
Epoch 125/200
73/73 - 94s - loss: 1.3094 - val_loss: 2.9608 - 94s/epoch - 1s/step
Epoch 126/200
73/73 - 96s - loss: 1.2475 - val_loss: 2.9589 - 96s/epoch - 1s/step
Epoch 127/200
73/73 - 96s - loss: 1.2110 - val_loss: 3.4439 - 96s/epoch - 1s/step
Epoch 128/200
73/73 - 96s - loss: 2.5630 - val_loss: 325.0197 - 96s/epoch - 1s/step
Epoch 129/200
73/73 - 95s - loss: 25.9191 - val_loss: 999.6627 - 95s/epoch - 1s/step
Epoch 130/200
73/73 - 94s - loss: 6.2687 - val_loss: 53.2040 - 94s/epoch - 1s/step
Epoch 131/200
73/73 - 95s - loss: 3.9442 - val_loss: 51.2671 - 95s/epoch - 1s/step
Epoch 132/200
73/73 - 96s - loss: 2.3978 - val_loss: 33.3704 - 96s/epoch - 1s/step
Epoch 133/200
73/73 - 96s - loss: 1.8026 - val_loss: 41.1881 - 96s/epoch - 1s/step
Epoch 134/200
73/73 - 96s - loss: 1.6267 - val_loss: 23.5376 - 96s/epoch - 1s/step
Epoch 135/200
73/73 - 95s - loss: 1.5547 - val_loss: 6.4663 - 95s/epoch - 1s/step
Epoch 136/200
73/73 - 95s - loss: 1.4419 - val_loss: 5.2687 - 95s/epoch - 1s/step
Epoch 137/200
73/73 - 95s - loss: 1.4146 - val_loss: 3.5053 - 95s/epoch - 1s/step
Epoch 138/200
73/73 - 96s - loss: 1.3146 - val_loss: 3.4819 - 96s/epoch - 1s/step
Epoch 139/200
73/73 - 97s - loss: 1.2534 - val_loss: 4.1419 - 97s/epoch - 1s/step
Epoch 140/200
73/73 - 96s - loss: 1.1801 - val_loss: 2.7963 - 96s/epoch - 1s/step
Epoch 141/200
73/73 - 96s - loss: 1.1633 - val_loss: 2.4951 - 96s/epoch - 1s/step
Epoch 142/200
73/73 - 97s - loss: 1.1460 - val_loss: 3.0280 - 97s/epoch - 1s/step
Epoch 143/200
73/73 - 94s - loss: 1.2601 - val_loss: 2.8638 - 94s/epoch - 1s/step
Epoch 144/200
73/73 - 95s - loss: 1.1496 - val_loss: 6.0536 - 95s/epoch - 1s/step
Epoch 145/200
73/73 - 96s - loss: 1.1411 - val_loss: 3.2317 - 96s/epoch - 1s/step
Epoch 146/200
73/73 - 97s - loss: 1.1479 - val_loss: 3.9025 - 97s/epoch - 1s/step
Epoch 147/200
73/73 - 97s - loss: 1.2503 - val_loss: 4.3836 - 97s/epoch - 1s/step
Epoch 148/200
73/73 - 96s - loss: 1.6051 - val_loss: 206.6052 - 96s/epoch - 1s/step
Epoch 149/200
73/73 - 96s - loss: 4.9437 - val_loss: 337.7014 - 96s/epoch - 1s/step
Epoch 150/200
73/73 - 97s - loss: 8.8177 - val_loss: 421.1735 - 97s/epoch - 1s/step
Epoch 151/200
73/73 - 97s - loss: 3.9863 - val_loss: 254.9996 - 97s/epoch - 1s/step
Epoch 152/200
73/73 - 95s - loss: 2.7394 - val_loss: 51.3412 - 95s/epoch - 1s/step
Epoch 153/200
73/73 - 95s - loss: 1.5330 - val_loss: 53.2984 - 95s/epoch - 1s/step
Epoch 154/200
73/73 - 97s - loss: 1.3337 - val_loss: 36.9732 - 97s/epoch - 1s/step
Epoch 155/200
73/73 - 97s - loss: 1.1907 - val_loss: 4.2864 - 97s/epoch - 1s/step
Epoch 156/200
73/73 - 96s - loss: 1.1505 - val_loss: 3.2633 - 96s/epoch - 1s/step
Epoch 157/200
73/73 - 93s - loss: 1.0597 - val_loss: 2.5859 - 93s/epoch - 1s/step
Epoch 158/200
73/73 - 95s - loss: 0.9877 - val_loss: 6.2117 - 95s/epoch - 1s/step
Epoch 159/200
73/73 - 97s - loss: 1.0193 - val_loss: 2.6362 - 97s/epoch - 1s/step
Epoch 160/200
73/73 - 96s - loss: 1.0646 - val_loss: 5.1920 - 96s/epoch - 1s/step
Epoch 161/200
73/73 - 95s - loss: 0.9788 - val_loss: 2.1195 - 95s/epoch - 1s/step
Epoch 162/200
73/73 - 96s - loss: 1.0106 - val_loss: 2.6315 - 96s/epoch - 1s/step
Epoch 163/200
73/73 - 96s - loss: 1.0136 - val_loss: 3.7204 - 96s/epoch - 1s/step
Epoch 164/200
73/73 - 97s - loss: 0.9619 - val_loss: 2.5537 - 97s/epoch - 1s/step
Epoch 165/200
73/73 - 95s - loss: 1.0020 - val_loss: 3.0945 - 95s/epoch - 1s/step
Epoch 166/200
73/73 - 96s - loss: 0.9586 - val_loss: 3.7152 - 96s/epoch - 1s/step
Epoch 167/200
73/73 - 96s - loss: 0.9175 - val_loss: 2.5757 - 96s/epoch - 1s/step
Epoch 168/200
73/73 - 95s - loss: 0.8376 - val_loss: 2.2989 - 95s/epoch - 1s/step
Epoch 169/200
73/73 - 96s - loss: 0.8377 - val_loss: 2.0102 - 96s/epoch - 1s/step
Epoch 170/200
73/73 - 96s - loss: 0.8592 - val_loss: 2.2351 - 96s/epoch - 1s/step
Epoch 171/200
73/73 - 95s - loss: 0.8823 - val_loss: 2.9466 - 95s/epoch - 1s/step
Epoch 172/200
73/73 - 96s - loss: 0.8528 - val_loss: 2.7677 - 96s/epoch - 1s/step
Epoch 173/200
73/73 - 95s - loss: 0.8432 - val_loss: 4.5833 - 95s/epoch - 1s/step
Epoch 174/200
73/73 - 96s - loss: 0.8305 - val_loss: 2.8642 - 96s/epoch - 1s/step
Epoch 175/200
73/73 - 95s - loss: 0.7856 - val_loss: 1.9917 - 95s/epoch - 1s/step
Epoch 176/200
73/73 - 96s - loss: 0.8801 - val_loss: 1.9370 - 96s/epoch - 1s/step
Epoch 177/200
73/73 - 95s - loss: 0.7982 - val_loss: 4.9511 - 95s/epoch - 1s/step
Epoch 178/200
73/73 - 94s - loss: 0.8385 - val_loss: 2.9041 - 94s/epoch - 1s/step
Epoch 179/200
73/73 - 94s - loss: 0.7592 - val_loss: 2.3349 - 94s/epoch - 1s/step
Epoch 180/200
73/73 - 94s - loss: 3.3282 - val_loss: 218.2506 - 94s/epoch - 1s/step
Epoch 181/200
73/73 - 95s - loss: 22.5534 - val_loss: 463.3814 - 95s/epoch - 1s/step
Epoch 182/200
73/73 - 95s - loss: 3.5113 - val_loss: 169.3695 - 95s/epoch - 1s/step
Epoch 183/200
73/73 - 96s - loss: 2.1703 - val_loss: 51.1677 - 96s/epoch - 1s/step
Epoch 184/200
73/73 - 96s - loss: 5.0184 - val_loss: 402.9282 - 96s/epoch - 1s/step
Epoch 185/200
73/73 - 95s - loss: 2.3129 - val_loss: 220.9970 - 95s/epoch - 1s/step
Epoch 186/200
73/73 - 96s - loss: 1.4854 - val_loss: 134.1424 - 96s/epoch - 1s/step
Epoch 187/200
73/73 - 95s - loss: 1.1706 - val_loss: 14.0083 - 95s/epoch - 1s/step
Epoch 188/200
73/73 - 94s - loss: 0.9889 - val_loss: 3.0963 - 94s/epoch - 1s/step
Epoch 189/200
73/73 - 95s - loss: 0.9052 - val_loss: 2.9023 - 95s/epoch - 1s/step
Epoch 190/200
73/73 - 95s - loss: 0.8801 - val_loss: 1.8475 - 95s/epoch - 1s/step
Epoch 191/200
73/73 - 96s - loss: 0.8522 - val_loss: 2.9784 - 96s/epoch - 1s/step
Epoch 192/200
73/73 - 97s - loss: 0.8794 - val_loss: 2.3028 - 97s/epoch - 1s/step
Epoch 193/200
73/73 - 94s - loss: 0.8757 - val_loss: 1.7287 - 94s/epoch - 1s/step
Epoch 194/200
73/73 - 93s - loss: 0.8016 - val_loss: 1.9665 - 93s/epoch - 1s/step
Epoch 195/200
73/73 - 94s - loss: 0.7320 - val_loss: 1.7009 - 94s/epoch - 1s/step
Epoch 196/200
73/73 - 95s - loss: 0.7360 - val_loss: 1.6761 - 95s/epoch - 1s/step
Epoch 197/200
73/73 - 95s - loss: 0.8057 - val_loss: 3.8904 - 95s/epoch - 1s/step
Epoch 198/200
73/73 - 95s - loss: 0.7203 - val_loss: 3.8012 - 95s/epoch - 1s/step
Epoch 199/200
73/73 - 95s - loss: 0.7803 - val_loss: 2.1196 - 95s/epoch - 1s/step
Epoch 200/200
73/73 - 95s - loss: 0.7629 - val_loss: 1.9280 - 95s/epoch - 1s/step
Test Loss (MSE): 1.9431846141815186
Model saved as: models/incept_surface_0904_0626_200ep_1.9432.keras
Training history saved as: models/history_0904_0626_200ep_1.9432.json
