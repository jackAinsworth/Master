nohup: ignoring input
2025-04-06 16:55:10.972511: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-06 16:55:11.020898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-06 16:55:13.819855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46551 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:61:00.0, compute capability: 8.9
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 100, 100, 3)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 100, 100, 64)         4864      ['input_1[0][0]']             
                                                                                                  
 conv2d_2 (Conv2D)           (None, 100, 100, 256)        16640     ['conv2d[0][0]']              
                                                                                                  
 conv2d_4 (Conv2D)           (None, 100, 100, 256)        16640     ['conv2d[0][0]']              
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 100, 100, 64)         0         ['conv2d[0][0]']              
 D)                                                                                               
                                                                                                  
 conv2d_1 (Conv2D)           (None, 100, 100, 256)        16640     ['conv2d[0][0]']              
                                                                                                  
 conv2d_3 (Conv2D)           (None, 100, 100, 256)        590080    ['conv2d_2[0][0]']            
                                                                                                  
 conv2d_5 (Conv2D)           (None, 100, 100, 256)        1638656   ['conv2d_4[0][0]']            
                                                                                                  
 conv2d_6 (Conv2D)           (None, 100, 100, 256)        16640     ['max_pooling2d[0][0]']       
                                                                                                  
 concatenate (Concatenate)   (None, 100, 100, 1024)       0         ['conv2d_1[0][0]',            
                                                                     'conv2d_3[0][0]',            
                                                                     'conv2d_5[0][0]',            
                                                                     'conv2d_6[0][0]']            
                                                                                                  
 conv2d_7 (Conv2D)           (None, 100, 100, 128)        1179776   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization (Batch  (None, 100, 100, 128)        512       ['conv2d_7[0][0]']            
 Normalization)                                                                                   
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 34, 34, 128)          0         ['batch_normalization[0][0]'] 
 g2D)                                                                                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 34, 34, 256)          33024     ['max_pooling2d_1[0][0]']     
                                                                                                  
 conv2d_11 (Conv2D)          (None, 34, 34, 256)          33024     ['max_pooling2d_1[0][0]']     
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 34, 34, 128)          0         ['max_pooling2d_1[0][0]']     
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 34, 34, 256)          33024     ['max_pooling2d_1[0][0]']     
                                                                                                  
 conv2d_10 (Conv2D)          (None, 34, 34, 256)          590080    ['conv2d_9[0][0]']            
                                                                                                  
 conv2d_12 (Conv2D)          (None, 34, 34, 256)          1638656   ['conv2d_11[0][0]']           
                                                                                                  
 conv2d_13 (Conv2D)          (None, 34, 34, 256)          33024     ['max_pooling2d_2[0][0]']     
                                                                                                  
 concatenate_1 (Concatenate  (None, 34, 34, 1024)         0         ['conv2d_8[0][0]',            
 )                                                                   'conv2d_10[0][0]',           
                                                                     'conv2d_12[0][0]',           
                                                                     'conv2d_13[0][0]']           
                                                                                                  
 conv2d_14 (Conv2D)          (None, 34, 34, 512)          4719104   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_1 (Bat  (None, 34, 34, 512)          2048      ['conv2d_14[0][0]']           
 chNormalization)                                                                                 
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 12, 12, 512)          0         ['batch_normalization_1[0][0]'
 g2D)                                                               ]                             
                                                                                                  
 conv2d_16 (Conv2D)          (None, 12, 12, 256)          131328    ['max_pooling2d_3[0][0]']     
                                                                                                  
 conv2d_18 (Conv2D)          (None, 12, 12, 256)          131328    ['max_pooling2d_3[0][0]']     
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 12, 12, 512)          0         ['max_pooling2d_3[0][0]']     
 g2D)                                                                                             
                                                                                                  
 conv2d_15 (Conv2D)          (None, 12, 12, 256)          131328    ['max_pooling2d_3[0][0]']     
                                                                                                  
 conv2d_17 (Conv2D)          (None, 12, 12, 256)          590080    ['conv2d_16[0][0]']           
                                                                                                  
 conv2d_19 (Conv2D)          (None, 12, 12, 256)          1638656   ['conv2d_18[0][0]']           
                                                                                                  
 conv2d_20 (Conv2D)          (None, 12, 12, 256)          131328    ['max_pooling2d_4[0][0]']     
                                                                                                  
 concatenate_2 (Concatenate  (None, 12, 12, 1024)         0         ['conv2d_15[0][0]',           
 )                                                                   'conv2d_17[0][0]',           
                                                                     'conv2d_19[0][0]',           
                                                                     'conv2d_20[0][0]']           
                                                                                                  
 conv2d_21 (Conv2D)          (None, 12, 12, 256)          2359552   ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 12, 12, 256)          1024      ['conv2d_21[0][0]']           
 chNormalization)                                                                                 
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 4, 4, 256)            0         ['batch_normalization_2[0][0]'
 g2D)                                                               ]                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 4, 4, 256)            65792     ['max_pooling2d_5[0][0]']     
                                                                                                  
 conv2d_25 (Conv2D)          (None, 4, 4, 256)            65792     ['max_pooling2d_5[0][0]']     
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 4, 4, 256)            0         ['max_pooling2d_5[0][0]']     
 g2D)                                                                                             
                                                                                                  
 conv2d_22 (Conv2D)          (None, 4, 4, 256)            65792     ['max_pooling2d_5[0][0]']     
                                                                                                  
 conv2d_24 (Conv2D)          (None, 4, 4, 256)            590080    ['conv2d_23[0][0]']           
                                                                                                  
 conv2d_26 (Conv2D)          (None, 4, 4, 256)            1638656   ['conv2d_25[0][0]']           
                                                                                                  
 conv2d_27 (Conv2D)          (None, 4, 4, 256)            65792     ['max_pooling2d_6[0][0]']     
                                                                                                  
 concatenate_3 (Concatenate  (None, 4, 4, 1024)           0         ['conv2d_22[0][0]',           
 )                                                                   'conv2d_24[0][0]',           
                                                                     'conv2d_26[0][0]',           
                                                                     'conv2d_27[0][0]']           
                                                                                                  
 conv2d_28 (Conv2D)          (None, 4, 4, 512)            4719104   ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_3 (Bat  (None, 4, 4, 512)            2048      ['conv2d_28[0][0]']           
 chNormalization)                                                                                 
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 2, 2, 512)            0         ['batch_normalization_3[0][0]'
 g2D)                                                               ]                             
                                                                                                  
 flatten (Flatten)           (None, 2048)                 0         ['max_pooling2d_7[0][0]']     
                                                                                                  
 dense (Dense)               (None, 192)                  393408    ['flatten[0][0]']             
                                                                                                  
 reshape (Reshape)           (None, 8, 8, 3)              0         ['dense[0][0]']               
                                                                                                  
==================================================================================================
Total params: 23283520 (88.82 MB)
Trainable params: 23280704 (88.81 MB)
Non-trainable params: 2816 (11.00 KB)
__________________________________________________________________________________________________
Training set: (56000, 100, 100, 3) (56000, 8, 8, 3)
Validation set: (12000, 100, 100, 3) (12000, 8, 8, 3)
Test set: (12000, 100, 100, 3) (12000, 8, 8, 3)
Epoch 1/200
2025-04-06 16:57:49.293200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
2025-04-06 16:57:57.971416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2025-04-06 16:57:58.097823: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1540c7ab30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2025-04-06 16:57:58.097862: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX 6000 Ada Generation, Compute Capability 8.9
2025-04-06 16:57:58.213338: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2025-04-06 16:57:58.783212: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2025-04-06 16:58:10.641172: W tensorflow/core/kernels/gpu_utils.cc:50] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
292/292 - 408s - loss: 356.8238 - val_loss: 2835.4121 - 408s/epoch - 1s/step
Epoch 2/200
292/292 - 334s - loss: 232.9171 - val_loss: 1459.5348 - 334s/epoch - 1s/step
Epoch 3/200
292/292 - 331s - loss: 137.6505 - val_loss: 371.8448 - 331s/epoch - 1s/step
Epoch 4/200
292/292 - 329s - loss: 80.2099 - val_loss: 4059.4419 - 329s/epoch - 1s/step
Epoch 5/200
292/292 - 329s - loss: 58.5442 - val_loss: 393.8595 - 329s/epoch - 1s/step
Epoch 6/200
292/292 - 328s - loss: 44.9496 - val_loss: 223.5278 - 328s/epoch - 1s/step
Epoch 7/200
292/292 - 327s - loss: 40.1321 - val_loss: 115.7433 - 327s/epoch - 1s/step
Epoch 8/200
292/292 - 327s - loss: 33.6333 - val_loss: 274.8069 - 327s/epoch - 1s/step
Epoch 9/200
292/292 - 327s - loss: 28.7393 - val_loss: 441.0184 - 327s/epoch - 1s/step
Epoch 10/200
292/292 - 326s - loss: 33.9566 - val_loss: 347.1859 - 326s/epoch - 1s/step
Epoch 11/200
292/292 - 325s - loss: 24.4610 - val_loss: 302.3301 - 325s/epoch - 1s/step
Epoch 12/200
292/292 - 325s - loss: 31.1881 - val_loss: 214.4368 - 325s/epoch - 1s/step
Epoch 13/200
292/292 - 324s - loss: 21.9642 - val_loss: 184.2131 - 324s/epoch - 1s/step
Epoch 14/200
292/292 - 324s - loss: 19.4574 - val_loss: 130.5654 - 324s/epoch - 1s/step
Epoch 15/200
292/292 - 324s - loss: 15.7064 - val_loss: 42.3354 - 324s/epoch - 1s/step
Epoch 16/200
292/292 - 323s - loss: 14.7443 - val_loss: 226.0499 - 323s/epoch - 1s/step
Epoch 17/200
292/292 - 323s - loss: 17.1996 - val_loss: 244.2290 - 323s/epoch - 1s/step
Epoch 18/200
292/292 - 323s - loss: 13.1431 - val_loss: 38.4699 - 323s/epoch - 1s/step
Epoch 19/200
292/292 - 323s - loss: 13.0871 - val_loss: 309.0962 - 323s/epoch - 1s/step
Epoch 20/200
292/292 - 322s - loss: 11.6881 - val_loss: 97.3398 - 322s/epoch - 1s/step
Epoch 21/200
292/292 - 323s - loss: 9.6094 - val_loss: 20.7359 - 323s/epoch - 1s/step
Epoch 22/200
292/292 - 322s - loss: 9.0284 - val_loss: 194.0700 - 322s/epoch - 1s/step
Epoch 23/200
292/292 - 322s - loss: 8.3388 - val_loss: 105.4052 - 322s/epoch - 1s/step
Epoch 24/200
292/292 - 323s - loss: 11.7487 - val_loss: 65.1274 - 323s/epoch - 1s/step
Epoch 25/200
292/292 - 323s - loss: 7.7721 - val_loss: 40.2078 - 323s/epoch - 1s/step
Epoch 26/200
292/292 - 322s - loss: 6.9267 - val_loss: 31.1258 - 322s/epoch - 1s/step
Epoch 27/200
292/292 - 322s - loss: 11.6237 - val_loss: 367.0821 - 322s/epoch - 1s/step
Epoch 28/200
292/292 - 322s - loss: 6.2109 - val_loss: 39.9598 - 322s/epoch - 1s/step
Epoch 29/200
292/292 - 322s - loss: 7.2771 - val_loss: 41.2250 - 322s/epoch - 1s/step
Epoch 30/200
292/292 - 321s - loss: 7.1250 - val_loss: 254.7411 - 321s/epoch - 1s/step
Epoch 31/200
292/292 - 321s - loss: 5.1035 - val_loss: 16.7321 - 321s/epoch - 1s/step
Epoch 32/200
292/292 - 322s - loss: 4.7228 - val_loss: 10.6812 - 322s/epoch - 1s/step
Epoch 33/200
292/292 - 321s - loss: 4.0567 - val_loss: 146.0417 - 321s/epoch - 1s/step
Epoch 34/200
292/292 - 321s - loss: 7.5571 - val_loss: 412.3875 - 321s/epoch - 1s/step
Epoch 35/200
292/292 - 321s - loss: 7.6029 - val_loss: 21.8833 - 321s/epoch - 1s/step
Epoch 36/200
292/292 - 320s - loss: 4.1808 - val_loss: 13.6466 - 320s/epoch - 1s/step
Epoch 37/200
292/292 - 321s - loss: 3.6379 - val_loss: 71.6568 - 321s/epoch - 1s/step
Epoch 38/200
292/292 - 320s - loss: 5.9965 - val_loss: 821.4099 - 320s/epoch - 1s/step
Epoch 39/200
292/292 - 320s - loss: 5.0158 - val_loss: 9.3856 - 320s/epoch - 1s/step
Epoch 40/200
292/292 - 320s - loss: 3.4468 - val_loss: 7.6453 - 320s/epoch - 1s/step
Epoch 41/200
292/292 - 320s - loss: 3.8441 - val_loss: 83.9261 - 320s/epoch - 1s/step
Epoch 42/200
292/292 - 321s - loss: 3.6687 - val_loss: 86.8077 - 321s/epoch - 1s/step
Epoch 43/200
292/292 - 321s - loss: 3.9880 - val_loss: 37.7956 - 321s/epoch - 1s/step
Epoch 44/200
292/292 - 321s - loss: 6.2855 - val_loss: 365.9486 - 321s/epoch - 1s/step
Epoch 45/200
292/292 - 319s - loss: 5.9576 - val_loss: 298.1076 - 319s/epoch - 1s/step
Epoch 46/200
292/292 - 319s - loss: 3.3230 - val_loss: 11.7613 - 319s/epoch - 1s/step
Epoch 47/200
292/292 - 320s - loss: 2.0537 - val_loss: 37.6057 - 320s/epoch - 1s/step
Epoch 48/200
292/292 - 320s - loss: 2.2558 - val_loss: 77.1291 - 320s/epoch - 1s/step
Epoch 49/200
292/292 - 319s - loss: 5.5035 - val_loss: 1268.9626 - 319s/epoch - 1s/step
Epoch 50/200
292/292 - 319s - loss: 3.2047 - val_loss: 16.3740 - 319s/epoch - 1s/step
Epoch 51/200
292/292 - 319s - loss: 1.9117 - val_loss: 7.6382 - 319s/epoch - 1s/step
Epoch 52/200
292/292 - 320s - loss: 2.3430 - val_loss: 197.7544 - 320s/epoch - 1s/step
Epoch 53/200
292/292 - 319s - loss: 2.0217 - val_loss: 12.4436 - 319s/epoch - 1s/step
Epoch 54/200
292/292 - 319s - loss: 2.6158 - val_loss: 17.6621 - 319s/epoch - 1s/step
Epoch 55/200
292/292 - 319s - loss: 3.8648 - val_loss: 14.0893 - 319s/epoch - 1s/step
Epoch 56/200
292/292 - 319s - loss: 2.4631 - val_loss: 1456.4009 - 319s/epoch - 1s/step
Epoch 57/200
292/292 - 319s - loss: 5.7069 - val_loss: 10.6414 - 319s/epoch - 1s/step
Epoch 58/200
292/292 - 319s - loss: 1.6838 - val_loss: 5.6484 - 319s/epoch - 1s/step
Epoch 59/200
292/292 - 319s - loss: 1.5604 - val_loss: 9.6228 - 319s/epoch - 1s/step
Epoch 60/200
292/292 - 319s - loss: 1.3656 - val_loss: 3.8692 - 319s/epoch - 1s/step
Epoch 61/200
292/292 - 319s - loss: 1.2208 - val_loss: 7.9097 - 319s/epoch - 1s/step
Epoch 62/200
292/292 - 319s - loss: 4.5367 - val_loss: 24.2501 - 319s/epoch - 1s/step
Epoch 63/200
292/292 - 319s - loss: 5.1566 - val_loss: 132.2075 - 319s/epoch - 1s/step
Epoch 64/200
292/292 - 319s - loss: 1.7737 - val_loss: 6.7988 - 319s/epoch - 1s/step
Epoch 65/200
292/292 - 319s - loss: 1.2902 - val_loss: 4.5645 - 319s/epoch - 1s/step
Epoch 66/200
292/292 - 319s - loss: 1.1224 - val_loss: 2.7394 - 319s/epoch - 1s/step
Epoch 67/200
292/292 - 319s - loss: 0.9679 - val_loss: 4.7304 - 319s/epoch - 1s/step
Epoch 68/200
292/292 - 320s - loss: 0.9593 - val_loss: 3.6814 - 320s/epoch - 1s/step
Epoch 69/200
292/292 - 319s - loss: 0.9829 - val_loss: 7.8269 - 319s/epoch - 1s/step
Epoch 70/200
292/292 - 319s - loss: 7.1892 - val_loss: 20.5995 - 319s/epoch - 1s/step
Epoch 71/200
292/292 - 319s - loss: 2.1851 - val_loss: 6.9499 - 319s/epoch - 1s/step
Epoch 72/200
292/292 - 319s - loss: 1.5152 - val_loss: 18.3785 - 319s/epoch - 1s/step
Epoch 73/200
292/292 - 319s - loss: 1.2609 - val_loss: 451.0840 - 319s/epoch - 1s/step
Epoch 74/200
292/292 - 319s - loss: 3.7703 - val_loss: 159.5566 - 319s/epoch - 1s/step
Epoch 75/200
292/292 - 319s - loss: 2.4529 - val_loss: 5.9360 - 319s/epoch - 1s/step
Epoch 76/200
292/292 - 318s - loss: 1.1107 - val_loss: 3.9466 - 318s/epoch - 1s/step
Epoch 77/200
292/292 - 319s - loss: 0.9553 - val_loss: 3.0045 - 319s/epoch - 1s/step
Epoch 78/200
292/292 - 318s - loss: 0.8206 - val_loss: 4.7804 - 318s/epoch - 1s/step
Epoch 79/200
292/292 - 318s - loss: 0.8088 - val_loss: 4.5744 - 318s/epoch - 1s/step
Epoch 80/200
292/292 - 319s - loss: 2.0687 - val_loss: 195.5249 - 319s/epoch - 1s/step
Epoch 81/200
292/292 - 318s - loss: 3.8448 - val_loss: 50.0642 - 318s/epoch - 1s/step
Epoch 82/200
292/292 - 317s - loss: 3.3987 - val_loss: 131.1513 - 317s/epoch - 1s/step
Epoch 83/200
292/292 - 317s - loss: 1.0372 - val_loss: 3.5558 - 317s/epoch - 1s/step
Epoch 84/200
292/292 - 318s - loss: 0.8126 - val_loss: 2.8130 - 318s/epoch - 1s/step
Epoch 85/200
292/292 - 318s - loss: 0.7704 - val_loss: 2.7597 - 318s/epoch - 1s/step
Epoch 86/200
292/292 - 317s - loss: 0.7658 - val_loss: 5.4406 - 317s/epoch - 1s/step
Epoch 87/200
292/292 - 317s - loss: 0.7793 - val_loss: 18.2336 - 317s/epoch - 1s/step
Epoch 88/200
292/292 - 317s - loss: 6.0132 - val_loss: 172.7333 - 317s/epoch - 1s/step
Epoch 89/200
292/292 - 317s - loss: 1.6242 - val_loss: 3.3077 - 317s/epoch - 1s/step
Epoch 90/200
292/292 - 317s - loss: 0.9327 - val_loss: 24.8183 - 317s/epoch - 1s/step
Epoch 91/200
292/292 - 318s - loss: 0.8232 - val_loss: 2.3322 - 318s/epoch - 1s/step
Epoch 92/200
292/292 - 317s - loss: 0.7174 - val_loss: 2.2394 - 317s/epoch - 1s/step
Epoch 93/200
292/292 - 317s - loss: 0.7871 - val_loss: 95.5341 - 317s/epoch - 1s/step
Epoch 94/200
292/292 - 317s - loss: 1.3256 - val_loss: 19.0114 - 317s/epoch - 1s/step
Epoch 95/200
292/292 - 317s - loss: 3.2980 - val_loss: 105.4371 - 317s/epoch - 1s/step
Epoch 96/200
292/292 - 316s - loss: 2.6704 - val_loss: 18.1720 - 316s/epoch - 1s/step
Epoch 97/200
292/292 - 317s - loss: 1.1645 - val_loss: 4.2845 - 317s/epoch - 1s/step
Epoch 98/200
292/292 - 318s - loss: 0.7330 - val_loss: 1.9603 - 318s/epoch - 1s/step
Epoch 99/200
292/292 - 317s - loss: 0.6567 - val_loss: 2.1046 - 317s/epoch - 1s/step
Epoch 100/200
292/292 - 317s - loss: 0.6509 - val_loss: 2.3096 - 317s/epoch - 1s/step
Epoch 101/200
292/292 - 317s - loss: 0.6368 - val_loss: 2.0832 - 317s/epoch - 1s/step
Epoch 102/200
292/292 - 317s - loss: 0.6159 - val_loss: 2.5317 - 317s/epoch - 1s/step
Epoch 103/200
292/292 - 317s - loss: 0.7924 - val_loss: 16.9849 - 317s/epoch - 1s/step
Epoch 104/200
292/292 - 316s - loss: 4.8459 - val_loss: 11.4247 - 316s/epoch - 1s/step
Epoch 105/200
292/292 - 316s - loss: 1.2355 - val_loss: 6.0086 - 316s/epoch - 1s/step
Epoch 106/200
292/292 - 316s - loss: 1.8901 - val_loss: 12.4921 - 316s/epoch - 1s/step
Epoch 107/200
292/292 - 317s - loss: 0.7743 - val_loss: 2.2039 - 317s/epoch - 1s/step
Epoch 108/200
292/292 - 317s - loss: 0.7729 - val_loss: 46.2070 - 317s/epoch - 1s/step
Epoch 109/200
292/292 - 317s - loss: 1.0492 - val_loss: 4.0677 - 317s/epoch - 1s/step
Epoch 110/200
292/292 - 317s - loss: 0.6442 - val_loss: 1.9306 - 317s/epoch - 1s/step
Epoch 111/200
292/292 - 317s - loss: 0.5993 - val_loss: 2.1977 - 317s/epoch - 1s/step
Epoch 112/200
292/292 - 317s - loss: 0.5793 - val_loss: 1.7701 - 317s/epoch - 1s/step
Epoch 113/200
292/292 - 317s - loss: 2.3008 - val_loss: 164.7825 - 317s/epoch - 1s/step
Epoch 114/200
292/292 - 317s - loss: 2.4373 - val_loss: 119.8250 - 317s/epoch - 1s/step
Epoch 115/200
292/292 - 317s - loss: 1.3723 - val_loss: 14.8100 - 317s/epoch - 1s/step
Epoch 116/200
292/292 - 317s - loss: 1.0067 - val_loss: 5.3339 - 317s/epoch - 1s/step
Epoch 117/200
292/292 - 317s - loss: 0.6759 - val_loss: 110.7547 - 317s/epoch - 1s/step
Epoch 118/200
292/292 - 317s - loss: 0.6072 - val_loss: 2.0787 - 317s/epoch - 1s/step
Epoch 119/200
292/292 - 316s - loss: 0.5769 - val_loss: 2.3242 - 316s/epoch - 1s/step
Epoch 120/200
292/292 - 317s - loss: 0.5269 - val_loss: 2.4041 - 317s/epoch - 1s/step
Epoch 121/200
292/292 - 317s - loss: 0.5668 - val_loss: 1.8458 - 317s/epoch - 1s/step
Epoch 122/200
292/292 - 317s - loss: 0.5292 - val_loss: 1.9467 - 317s/epoch - 1s/step
Epoch 123/200
292/292 - 317s - loss: 0.5272 - val_loss: 1.8283 - 317s/epoch - 1s/step
Epoch 124/200
292/292 - 317s - loss: 0.5313 - val_loss: 1.8890 - 317s/epoch - 1s/step
Epoch 125/200
292/292 - 316s - loss: 3.9787 - val_loss: 835.2073 - 316s/epoch - 1s/step
Epoch 126/200
292/292 - 316s - loss: 2.3912 - val_loss: 3.5753 - 316s/epoch - 1s/step
Epoch 127/200
292/292 - 317s - loss: 0.7832 - val_loss: 2.2175 - 317s/epoch - 1s/step
Epoch 128/200
292/292 - 316s - loss: 0.6040 - val_loss: 1.7258 - 316s/epoch - 1s/step
Epoch 129/200
292/292 - 316s - loss: 0.5423 - val_loss: 1.7029 - 316s/epoch - 1s/step
Epoch 130/200
292/292 - 317s - loss: 1.2960 - val_loss: 394.3321 - 317s/epoch - 1s/step
Epoch 131/200
292/292 - 316s - loss: 2.6606 - val_loss: 44.8805 - 316s/epoch - 1s/step
Epoch 132/200
292/292 - 317s - loss: 0.7404 - val_loss: 11.9324 - 317s/epoch - 1s/step
Epoch 133/200
292/292 - 316s - loss: 0.5591 - val_loss: 1.6376 - 316s/epoch - 1s/step
Epoch 134/200
292/292 - 317s - loss: 0.5151 - val_loss: 1.6542 - 317s/epoch - 1s/step
Epoch 135/200
292/292 - 317s - loss: 0.4902 - val_loss: 1.4481 - 317s/epoch - 1s/step
Epoch 136/200
292/292 - 317s - loss: 0.4881 - val_loss: 2.0574 - 317s/epoch - 1s/step
Epoch 137/200
292/292 - 317s - loss: 0.4738 - val_loss: 1.7724 - 317s/epoch - 1s/step
Epoch 138/200
292/292 - 317s - loss: 0.4617 - val_loss: 1.6361 - 317s/epoch - 1s/step
Epoch 139/200
292/292 - 317s - loss: 0.4796 - val_loss: 1.9731 - 317s/epoch - 1s/step
Epoch 140/200
292/292 - 317s - loss: 3.2345 - val_loss: 1288.8699 - 317s/epoch - 1s/step
Epoch 141/200
292/292 - 316s - loss: 3.1176 - val_loss: 3.4835 - 316s/epoch - 1s/step
Epoch 142/200
292/292 - 316s - loss: 0.6527 - val_loss: 1.8283 - 316s/epoch - 1s/step
Epoch 143/200
292/292 - 316s - loss: 0.5375 - val_loss: 1.5138 - 316s/epoch - 1s/step
Epoch 144/200
292/292 - 316s - loss: 0.4647 - val_loss: 1.8558 - 316s/epoch - 1s/step
Epoch 145/200
292/292 - 316s - loss: 0.4549 - val_loss: 1.4560 - 316s/epoch - 1s/step
Epoch 146/200
292/292 - 316s - loss: 0.4569 - val_loss: 1.8261 - 316s/epoch - 1s/step
Epoch 147/200
292/292 - 316s - loss: 0.4505 - val_loss: 1.4507 - 316s/epoch - 1s/step
Epoch 148/200
292/292 - 316s - loss: 0.4612 - val_loss: 1.4811 - 316s/epoch - 1s/step
Epoch 149/200
292/292 - 316s - loss: 0.4417 - val_loss: 1.5829 - 316s/epoch - 1s/step
Epoch 150/200
292/292 - 315s - loss: 0.4414 - val_loss: 1.4432 - 315s/epoch - 1s/step
Epoch 151/200
292/292 - 316s - loss: 0.4659 - val_loss: 47.9445 - 316s/epoch - 1s/step
Epoch 152/200
292/292 - 315s - loss: 3.8033 - val_loss: 339.2711 - 315s/epoch - 1s/step
Epoch 153/200
292/292 - 316s - loss: 1.2443 - val_loss: 4.3236 - 316s/epoch - 1s/step
Epoch 154/200
292/292 - 315s - loss: 0.5208 - val_loss: 26.8978 - 315s/epoch - 1s/step
Epoch 155/200
292/292 - 316s - loss: 0.4714 - val_loss: 1.5088 - 316s/epoch - 1s/step
Epoch 156/200
292/292 - 315s - loss: 0.4317 - val_loss: 1.4920 - 315s/epoch - 1s/step
Epoch 157/200
292/292 - 315s - loss: 0.4149 - val_loss: 1.3579 - 315s/epoch - 1s/step
Epoch 158/200
292/292 - 315s - loss: 0.4288 - val_loss: 1.2604 - 315s/epoch - 1s/step
Epoch 159/200
292/292 - 315s - loss: 0.4118 - val_loss: 2.9485 - 315s/epoch - 1s/step
Epoch 160/200
292/292 - 315s - loss: 0.4234 - val_loss: 1.5310 - 315s/epoch - 1s/step
Epoch 161/200
292/292 - 316s - loss: 0.4304 - val_loss: 1.5252 - 316s/epoch - 1s/step
Epoch 162/200
292/292 - 315s - loss: 0.5106 - val_loss: 9.4296 - 315s/epoch - 1s/step
Epoch 163/200
292/292 - 316s - loss: 3.9360 - val_loss: 123.6997 - 316s/epoch - 1s/step
Epoch 164/200
292/292 - 315s - loss: 0.8294 - val_loss: 20.8470 - 315s/epoch - 1s/step
Epoch 165/200
292/292 - 315s - loss: 0.6558 - val_loss: 3.6917 - 315s/epoch - 1s/step
Epoch 166/200
292/292 - 315s - loss: 0.4560 - val_loss: 1.6397 - 315s/epoch - 1s/step
Epoch 167/200
292/292 - 315s - loss: 0.4104 - val_loss: 1.2736 - 315s/epoch - 1s/step
Epoch 168/200
292/292 - 316s - loss: 0.4004 - val_loss: 1.5207 - 316s/epoch - 1s/step
Epoch 169/200
292/292 - 315s - loss: 0.3814 - val_loss: 1.5488 - 315s/epoch - 1s/step
Epoch 170/200
292/292 - 315s - loss: 0.3876 - val_loss: 1.5769 - 315s/epoch - 1s/step
Epoch 171/200
292/292 - 315s - loss: 0.4176 - val_loss: 1.2679 - 315s/epoch - 1s/step
Epoch 172/200
292/292 - 315s - loss: 0.3907 - val_loss: 1.2948 - 315s/epoch - 1s/step
Epoch 173/200
292/292 - 315s - loss: 0.3973 - val_loss: 1.5783 - 315s/epoch - 1s/step
Epoch 174/200
292/292 - 315s - loss: 0.3883 - val_loss: 1.2795 - 315s/epoch - 1s/step
Epoch 175/200
292/292 - 314s - loss: 0.3744 - val_loss: 1.2665 - 314s/epoch - 1s/step
Epoch 176/200
292/292 - 315s - loss: 1.5399 - val_loss: 241.5197 - 315s/epoch - 1s/step
Epoch 177/200
292/292 - 314s - loss: 2.8991 - val_loss: 5.7105 - 314s/epoch - 1s/step
Epoch 178/200
292/292 - 315s - loss: 0.6717 - val_loss: 5.2167 - 315s/epoch - 1s/step
Epoch 179/200
292/292 - 314s - loss: 0.4541 - val_loss: 1.3941 - 314s/epoch - 1s/step
Epoch 180/200
292/292 - 314s - loss: 0.3934 - val_loss: 1.2430 - 314s/epoch - 1s/step
Epoch 181/200
292/292 - 314s - loss: 0.3653 - val_loss: 1.1706 - 314s/epoch - 1s/step
Epoch 182/200
292/292 - 314s - loss: 0.3700 - val_loss: 1.1951 - 314s/epoch - 1s/step
Epoch 183/200
292/292 - 313s - loss: 0.3665 - val_loss: 1.2676 - 313s/epoch - 1s/step
Epoch 184/200
292/292 - 314s - loss: 0.3668 - val_loss: 1.9848 - 314s/epoch - 1s/step
Epoch 185/200
292/292 - 315s - loss: 0.3884 - val_loss: 1.4444 - 315s/epoch - 1s/step
Epoch 186/200
292/292 - 314s - loss: 0.3777 - val_loss: 1.9406 - 314s/epoch - 1s/step
Epoch 187/200
292/292 - 314s - loss: 0.3789 - val_loss: 4.4756 - 314s/epoch - 1s/step
Epoch 188/200
292/292 - 314s - loss: 3.6997 - val_loss: 19.8990 - 314s/epoch - 1s/step
Epoch 189/200
292/292 - 314s - loss: 0.5462 - val_loss: 3.0996 - 314s/epoch - 1s/step
Epoch 190/200
292/292 - 314s - loss: 0.4304 - val_loss: 1.8621 - 314s/epoch - 1s/step
Epoch 191/200
292/292 - 314s - loss: 0.3965 - val_loss: 1.2882 - 314s/epoch - 1s/step
Epoch 192/200
292/292 - 314s - loss: 0.3598 - val_loss: 1.0900 - 314s/epoch - 1s/step
Epoch 193/200
292/292 - 314s - loss: 0.3516 - val_loss: 1.1111 - 314s/epoch - 1s/step
Epoch 194/200
292/292 - 314s - loss: 0.3674 - val_loss: 1.0415 - 314s/epoch - 1s/step
Epoch 195/200
292/292 - 314s - loss: 0.3513 - val_loss: 1.3345 - 314s/epoch - 1s/step
Epoch 196/200
292/292 - 314s - loss: 0.3462 - val_loss: 1.6574 - 314s/epoch - 1s/step
Epoch 197/200
292/292 - 314s - loss: 0.3364 - val_loss: 1.2219 - 314s/epoch - 1s/step
Epoch 198/200
292/292 - 314s - loss: 0.3515 - val_loss: 1.6528 - 314s/epoch - 1s/step
Epoch 199/200
292/292 - 314s - loss: 0.3359 - val_loss: 1.1310 - 314s/epoch - 1s/step
Epoch 200/200
292/292 - 314s - loss: 0.3338 - val_loss: 1.1065 - 314s/epoch - 1s/step
Test Loss (MSE): 1.1380723714828491
Model saved as: models/incept_surface_0704_1040_200ep_1.1381.keras
Training history saved as: models/history_0704_1040_200ep_1.1381.json
