nohup: ignoring input
2025-05-30 10:21:01.126066: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-30 10:21:01.457393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-30 10:21:07.605513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46551 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:61:00.0, compute capability: 8.9
2025-05-30 10:21:07.607190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46542 MB memory:  -> device: 1, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:3d:00.0, compute capability: 8.9
Number of devices: 2
Training set: (448000, 35, 35, 3) (448000, 10, 10, 3)
Validation set: (56000, 35, 35, 3) (56000, 10, 10, 3)
Test set: (56000, 35, 35, 3) (56000, 10, 10, 3)
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 35, 35, 3)]          0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 35, 35, 64)           4864      ['input_1[0][0]']             
                                                                                                  
 conv2d_2 (Conv2D)           (None, 35, 35, 64)           4160      ['conv2d[0][0]']              
                                                                                                  
 conv2d_4 (Conv2D)           (None, 35, 35, 64)           4160      ['conv2d[0][0]']              
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 35, 35, 64)           0         ['conv2d[0][0]']              
 D)                                                                                               
                                                                                                  
 conv2d_1 (Conv2D)           (None, 35, 35, 64)           4160      ['conv2d[0][0]']              
                                                                                                  
 conv2d_3 (Conv2D)           (None, 35, 35, 64)           36928     ['conv2d_2[0][0]']            
                                                                                                  
 conv2d_5 (Conv2D)           (None, 35, 35, 64)           102464    ['conv2d_4[0][0]']            
                                                                                                  
 conv2d_6 (Conv2D)           (None, 35, 35, 64)           4160      ['max_pooling2d[0][0]']       
                                                                                                  
 concatenate (Concatenate)   (None, 35, 35, 256)          0         ['conv2d_1[0][0]',            
                                                                     'conv2d_3[0][0]',            
                                                                     'conv2d_5[0][0]',            
                                                                     'conv2d_6[0][0]']            
                                                                                                  
 conv2d_7 (Conv2D)           (None, 35, 35, 128)          295040    ['concatenate[0][0]']         
                                                                                                  
 batch_normalization (Batch  (None, 35, 35, 128)          512       ['conv2d_7[0][0]']            
 Normalization)                                                                                   
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 12, 12, 128)          0         ['batch_normalization[0][0]'] 
 g2D)                                                                                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 12, 12, 256)          33024     ['max_pooling2d_1[0][0]']     
                                                                                                  
 conv2d_11 (Conv2D)          (None, 12, 12, 256)          33024     ['max_pooling2d_1[0][0]']     
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 12, 12, 128)          0         ['max_pooling2d_1[0][0]']     
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 12, 12, 256)          33024     ['max_pooling2d_1[0][0]']     
                                                                                                  
 conv2d_10 (Conv2D)          (None, 12, 12, 256)          590080    ['conv2d_9[0][0]']            
                                                                                                  
 conv2d_12 (Conv2D)          (None, 12, 12, 256)          1638656   ['conv2d_11[0][0]']           
                                                                                                  
 conv2d_13 (Conv2D)          (None, 12, 12, 256)          33024     ['max_pooling2d_2[0][0]']     
                                                                                                  
 concatenate_1 (Concatenate  (None, 12, 12, 1024)         0         ['conv2d_8[0][0]',            
 )                                                                   'conv2d_10[0][0]',           
                                                                     'conv2d_12[0][0]',           
                                                                     'conv2d_13[0][0]']           
                                                                                                  
 conv2d_14 (Conv2D)          (None, 12, 12, 512)          4719104   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_1 (Bat  (None, 12, 12, 512)          2048      ['conv2d_14[0][0]']           
 chNormalization)                                                                                 
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 4, 4, 512)            0         ['batch_normalization_1[0][0]'
 g2D)                                                               ]                             
                                                                                                  
 conv2d_16 (Conv2D)          (None, 4, 4, 256)            131328    ['max_pooling2d_3[0][0]']     
                                                                                                  
 conv2d_18 (Conv2D)          (None, 4, 4, 256)            131328    ['max_pooling2d_3[0][0]']     
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 4, 4, 512)            0         ['max_pooling2d_3[0][0]']     
 g2D)                                                                                             
                                                                                                  
 conv2d_15 (Conv2D)          (None, 4, 4, 256)            131328    ['max_pooling2d_3[0][0]']     
                                                                                                  
 conv2d_17 (Conv2D)          (None, 4, 4, 256)            590080    ['conv2d_16[0][0]']           
                                                                                                  
 conv2d_19 (Conv2D)          (None, 4, 4, 256)            1638656   ['conv2d_18[0][0]']           
                                                                                                  
 conv2d_20 (Conv2D)          (None, 4, 4, 256)            131328    ['max_pooling2d_4[0][0]']     
                                                                                                  
 concatenate_2 (Concatenate  (None, 4, 4, 1024)           0         ['conv2d_15[0][0]',           
 )                                                                   'conv2d_17[0][0]',           
                                                                     'conv2d_19[0][0]',           
                                                                     'conv2d_20[0][0]']           
                                                                                                  
 conv2d_21 (Conv2D)          (None, 4, 4, 256)            2359552   ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 4, 4, 256)            1024      ['conv2d_21[0][0]']           
 chNormalization)                                                                                 
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 2, 2, 256)            0         ['batch_normalization_2[0][0]'
 g2D)                                                               ]                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 2, 2, 256)            65792     ['max_pooling2d_5[0][0]']     
                                                                                                  
 conv2d_25 (Conv2D)          (None, 2, 2, 256)            65792     ['max_pooling2d_5[0][0]']     
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 2, 2, 256)            0         ['max_pooling2d_5[0][0]']     
 g2D)                                                                                             
                                                                                                  
 conv2d_22 (Conv2D)          (None, 2, 2, 256)            65792     ['max_pooling2d_5[0][0]']     
                                                                                                  
 conv2d_24 (Conv2D)          (None, 2, 2, 256)            590080    ['conv2d_23[0][0]']           
                                                                                                  
 conv2d_26 (Conv2D)          (None, 2, 2, 256)            1638656   ['conv2d_25[0][0]']           
                                                                                                  
 conv2d_27 (Conv2D)          (None, 2, 2, 256)            65792     ['max_pooling2d_6[0][0]']     
                                                                                                  
 concatenate_3 (Concatenate  (None, 2, 2, 1024)           0         ['conv2d_22[0][0]',           
 )                                                                   'conv2d_24[0][0]',           
                                                                     'conv2d_26[0][0]',           
                                                                     'conv2d_27[0][0]']           
                                                                                                  
 conv2d_28 (Conv2D)          (None, 2, 2, 512)            4719104   ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_3 (Bat  (None, 2, 2, 512)            2048      ['conv2d_28[0][0]']           
 chNormalization)                                                                                 
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 1, 1, 512)            0         ['batch_normalization_3[0][0]'
 g2D)                                                               ]                             
                                                                                                  
 flatten (Flatten)           (None, 512)                  0         ['max_pooling2d_7[0][0]']     
                                                                                                  
 dense (Dense)               (None, 300)                  153900    ['flatten[0][0]']             
                                                                                                  
 reshape (Reshape)           (None, 10, 10, 3)            0         ['dense[0][0]']               
                                                                                                  
==================================================================================================
Total params: 20020012 (76.37 MB)
Trainable params: 20017196 (76.36 MB)
Non-trainable params: 2816 (11.00 KB)
__________________________________________________________________________________________________
Epoch 1/130
2025-05-30 10:29:46.850665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
2025-05-30 10:29:46.854894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
2025-05-30 10:29:48.931687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2025-05-30 10:29:51.472056: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f04da76b880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2025-05-30 10:29:51.472100: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX 6000 Ada Generation, Compute Capability 8.9
2025-05-30 10:29:51.472107: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA RTX 6000 Ada Generation, Compute Capability 8.9
2025-05-30 10:29:51.599507: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2025-05-30 10:29:52.205070: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1750/1750 - 163s - loss: 325.4734 - mse: 251.9207 - val_loss: 270.5654 - val_mse: 205.9213 - 163s/epoch - 93ms/step
Epoch 2/130
1750/1750 - 124s - loss: 244.8070 - mse: 186.0417 - val_loss: 234.1325 - val_mse: 179.0506 - 124s/epoch - 71ms/step
Epoch 3/130
1750/1750 - 123s - loss: 223.0800 - mse: 170.0953 - val_loss: 217.7447 - val_mse: 165.3412 - 123s/epoch - 70ms/step
Epoch 4/130
1750/1750 - 126s - loss: 189.2399 - mse: 139.5871 - val_loss: 239.2986 - val_mse: 186.2038 - 126s/epoch - 72ms/step
Epoch 5/130
1750/1750 - 126s - loss: 164.8095 - mse: 117.8016 - val_loss: 171.4869 - val_mse: 124.4372 - 126s/epoch - 72ms/step
Epoch 6/130
1750/1750 - 127s - loss: 150.0381 - mse: 105.1050 - val_loss: 181.2343 - val_mse: 133.7199 - 127s/epoch - 72ms/step
Epoch 7/130
1750/1750 - 127s - loss: 134.5014 - mse: 91.8181 - val_loss: 153.0342 - val_mse: 107.7707 - 127s/epoch - 72ms/step
Epoch 8/130
1750/1750 - 128s - loss: 122.7361 - mse: 82.0933 - val_loss: 143.2383 - val_mse: 100.4772 - 128s/epoch - 73ms/step
Epoch 9/130
1750/1750 - 127s - loss: 112.1470 - mse: 73.5827 - val_loss: 120.0004 - val_mse: 81.0951 - 127s/epoch - 73ms/step
Epoch 10/130
1750/1750 - 129s - loss: 102.7212 - mse: 66.4023 - val_loss: 170.5307 - val_mse: 124.8090 - 129s/epoch - 74ms/step
Epoch 11/130
1750/1750 - 124s - loss: 94.6237 - mse: 60.4653 - val_loss: 143.8514 - val_mse: 101.9566 - 124s/epoch - 71ms/step
Epoch 12/130
1750/1750 - 124s - loss: 87.0721 - mse: 55.0286 - val_loss: 116.5915 - val_mse: 78.2181 - 124s/epoch - 71ms/step
Epoch 13/130
1750/1750 - 124s - loss: 79.9465 - mse: 49.9579 - val_loss: 104.0860 - val_mse: 68.8384 - 124s/epoch - 71ms/step
Epoch 14/130
1750/1750 - 125s - loss: 74.0836 - mse: 45.8833 - val_loss: 83.3126 - val_mse: 53.1226 - 125s/epoch - 72ms/step
Epoch 15/130
1750/1750 - 126s - loss: 68.7035 - mse: 42.1772 - val_loss: 78.0763 - val_mse: 49.6596 - 126s/epoch - 72ms/step
Epoch 16/130
1750/1750 - 126s - loss: 63.9142 - mse: 38.9315 - val_loss: 78.3976 - val_mse: 49.9283 - 126s/epoch - 72ms/step
Epoch 17/130
1750/1750 - 126s - loss: 60.3474 - mse: 36.5422 - val_loss: 71.0757 - val_mse: 44.9942 - 126s/epoch - 72ms/step
Epoch 18/130
1750/1750 - 125s - loss: 57.5856 - mse: 34.6877 - val_loss: 71.7944 - val_mse: 45.5602 - 125s/epoch - 72ms/step
Epoch 19/130
1750/1750 - 126s - loss: 55.7517 - mse: 33.4628 - val_loss: 68.2157 - val_mse: 43.1583 - 126s/epoch - 72ms/step
Epoch 20/130
1750/1750 - 126s - loss: 54.9562 - mse: 32.9240 - val_loss: 67.5383 - val_mse: 42.7032 - 126s/epoch - 72ms/step
Epoch 21/130
1750/1750 - 126s - loss: 93.2178 - mse: 60.1798 - val_loss: 150.4779 - val_mse: 105.8131 - 126s/epoch - 72ms/step
Epoch 22/130
1750/1750 - 124s - loss: 86.1621 - mse: 55.2678 - val_loss: 147.5773 - val_mse: 101.5037 - 124s/epoch - 71ms/step
Epoch 23/130
1750/1750 - 126s - loss: 79.9934 - mse: 51.0939 - val_loss: 104.2306 - val_mse: 69.3564 - 126s/epoch - 72ms/step
Epoch 24/130
1750/1750 - 125s - loss: 75.1718 - mse: 47.8445 - val_loss: 108.3077 - val_mse: 72.5388 - 125s/epoch - 72ms/step
Epoch 25/130
1750/1750 - 127s - loss: 71.0923 - mse: 45.0906 - val_loss: 88.2478 - val_mse: 58.2467 - 127s/epoch - 73ms/step
Epoch 26/130
1750/1750 - 128s - loss: 66.7129 - mse: 42.1830 - val_loss: 94.2612 - val_mse: 61.6834 - 128s/epoch - 73ms/step
Epoch 27/130
1750/1750 - 128s - loss: 62.9779 - mse: 39.6838 - val_loss: 77.6530 - val_mse: 50.3689 - 128s/epoch - 73ms/step
Epoch 28/130
1750/1750 - 128s - loss: 59.5550 - mse: 37.3899 - val_loss: 90.8994 - val_mse: 59.3875 - 128s/epoch - 73ms/step
Epoch 29/130
1750/1750 - 128s - loss: 56.5901 - mse: 35.4464 - val_loss: 84.1817 - val_mse: 55.2607 - 128s/epoch - 73ms/step
Epoch 30/130
1750/1750 - 126s - loss: 53.1404 - mse: 33.1398 - val_loss: 109.3509 - val_mse: 72.9491 - 126s/epoch - 72ms/step
Epoch 31/130
1750/1750 - 126s - loss: 50.0428 - mse: 31.1056 - val_loss: 82.3859 - val_mse: 53.4171 - 126s/epoch - 72ms/step
Epoch 32/130
1750/1750 - 127s - loss: 47.3353 - mse: 29.2967 - val_loss: 78.5931 - val_mse: 50.7687 - 127s/epoch - 72ms/step
Epoch 33/130
1750/1750 - 127s - loss: 44.1125 - mse: 27.2053 - val_loss: 94.3855 - val_mse: 61.8752 - 127s/epoch - 73ms/step
Epoch 34/130
1750/1750 - 127s - loss: 41.4849 - mse: 25.4685 - val_loss: 83.9873 - val_mse: 54.1488 - 127s/epoch - 72ms/step
Epoch 35/130
1750/1750 - 128s - loss: 38.2996 - mse: 23.4138 - val_loss: 85.6957 - val_mse: 54.8416 - 128s/epoch - 73ms/step
Epoch 36/130
1750/1750 - 129s - loss: 35.8639 - mse: 21.8316 - val_loss: 81.3165 - val_mse: 51.9777 - 129s/epoch - 74ms/step
Epoch 37/130
1750/1750 - 129s - loss: 33.6289 - mse: 20.3771 - val_loss: 90.8826 - val_mse: 58.7510 - 129s/epoch - 74ms/step
Epoch 38/130
1750/1750 - 128s - loss: 31.4633 - mse: 18.9752 - val_loss: 66.4939 - val_mse: 42.7196 - 128s/epoch - 73ms/step
Epoch 39/130
1750/1750 - 126s - loss: 28.8676 - mse: 17.3503 - val_loss: 73.9447 - val_mse: 46.8009 - 126s/epoch - 72ms/step
Epoch 40/130
1750/1750 - 126s - loss: 26.7955 - mse: 16.0324 - val_loss: 75.0904 - val_mse: 47.5412 - 126s/epoch - 72ms/step
Epoch 41/130
1750/1750 - 126s - loss: 24.5325 - mse: 14.6049 - val_loss: 70.9370 - val_mse: 45.3825 - 126s/epoch - 72ms/step
Epoch 42/130
1750/1750 - 125s - loss: 22.7991 - mse: 13.5108 - val_loss: 71.8971 - val_mse: 45.7430 - 125s/epoch - 71ms/step
Epoch 43/130
1750/1750 - 126s - loss: 20.8722 - mse: 12.3184 - val_loss: 71.9609 - val_mse: 45.8542 - 126s/epoch - 72ms/step
Epoch 44/130
1750/1750 - 125s - loss: 19.0589 - mse: 11.2017 - val_loss: 77.6124 - val_mse: 49.0399 - 125s/epoch - 71ms/step
Epoch 45/130
1750/1750 - 125s - loss: 17.5257 - mse: 10.2561 - val_loss: 64.2095 - val_mse: 40.8235 - 125s/epoch - 72ms/step
Epoch 46/130
1750/1750 - 125s - loss: 15.8831 - mse: 9.2650 - val_loss: 62.0947 - val_mse: 39.8579 - 125s/epoch - 71ms/step
Epoch 47/130
1750/1750 - 125s - loss: 14.7917 - mse: 8.5923 - val_loss: 56.9163 - val_mse: 36.7428 - 125s/epoch - 71ms/step
Epoch 48/130
1750/1750 - 125s - loss: 13.4260 - mse: 7.7630 - val_loss: 51.0418 - val_mse: 33.4148 - 125s/epoch - 72ms/step
Epoch 49/130
1750/1750 - 125s - loss: 12.4589 - mse: 7.1813 - val_loss: 52.2660 - val_mse: 34.0935 - 125s/epoch - 71ms/step
Epoch 50/130
1750/1750 - 125s - loss: 11.4360 - mse: 6.5709 - val_loss: 47.3773 - val_mse: 31.2970 - 125s/epoch - 71ms/step
Epoch 51/130
1750/1750 - 124s - loss: 10.6317 - mse: 6.0879 - val_loss: 52.1981 - val_mse: 34.0657 - 124s/epoch - 71ms/step
Epoch 52/130
1750/1750 - 124s - loss: 10.0041 - mse: 5.7180 - val_loss: 50.5291 - val_mse: 33.0404 - 124s/epoch - 71ms/step
Epoch 53/130
1750/1750 - 126s - loss: 9.4071 - mse: 5.3563 - val_loss: 48.4858 - val_mse: 31.8608 - 126s/epoch - 72ms/step
Epoch 54/130
1750/1750 - 126s - loss: 8.9329 - mse: 5.0799 - val_loss: 45.4717 - val_mse: 30.0915 - 126s/epoch - 72ms/step
Epoch 55/130
1750/1750 - 125s - loss: 8.5838 - mse: 4.8660 - val_loss: 45.3134 - val_mse: 30.0156 - 125s/epoch - 71ms/step
Epoch 56/130
1750/1750 - 126s - loss: 8.3334 - mse: 4.7233 - val_loss: 44.7929 - val_mse: 29.7456 - 126s/epoch - 72ms/step
Epoch 57/130
1750/1750 - 126s - loss: 8.1289 - mse: 4.6028 - val_loss: 44.7989 - val_mse: 29.7061 - 126s/epoch - 72ms/step
Epoch 58/130
1750/1750 - 125s - loss: 7.9909 - mse: 4.5180 - val_loss: 44.3315 - val_mse: 29.4458 - 125s/epoch - 72ms/step
Epoch 59/130
1750/1750 - 125s - loss: 7.9384 - mse: 4.4879 - val_loss: 44.2075 - val_mse: 29.3645 - 125s/epoch - 72ms/step
Epoch 60/130
1750/1750 - 126s - loss: 7.8736 - mse: 4.4476 - val_loss: 44.1312 - val_mse: 29.3223 - 126s/epoch - 72ms/step
Epoch 61/130
1750/1750 - 125s - loss: 33.5618 - mse: 20.4693 - val_loss: 88.6490 - val_mse: 57.7531 - 125s/epoch - 71ms/step
Epoch 62/130
1750/1750 - 125s - loss: 33.4354 - mse: 20.4751 - val_loss: 77.4032 - val_mse: 50.2984 - 125s/epoch - 71ms/step
Epoch 63/130
1750/1750 - 125s - loss: 32.3204 - mse: 19.7613 - val_loss: 85.2012 - val_mse: 55.9739 - 125s/epoch - 72ms/step
Epoch 64/130
1750/1750 - 125s - loss: 30.8906 - mse: 18.8522 - val_loss: 79.5195 - val_mse: 51.5038 - 125s/epoch - 72ms/step
Epoch 65/130
1750/1750 - 126s - loss: 29.8538 - mse: 18.1598 - val_loss: 72.9192 - val_mse: 47.2806 - 126s/epoch - 72ms/step
Epoch 66/130
1750/1750 - 126s - loss: 28.0385 - mse: 17.0418 - val_loss: 83.1550 - val_mse: 53.9908 - 126s/epoch - 72ms/step
Epoch 67/130
1750/1750 - 126s - loss: 27.2425 - mse: 16.5378 - val_loss: 69.7744 - val_mse: 44.6207 - 126s/epoch - 72ms/step
Epoch 68/130
1750/1750 - 126s - loss: 26.1736 - mse: 15.8346 - val_loss: 71.5392 - val_mse: 46.0074 - 126s/epoch - 72ms/step
Epoch 69/130
1750/1750 - 125s - loss: 25.0128 - mse: 15.1121 - val_loss: 101.1458 - val_mse: 65.3637 - 125s/epoch - 72ms/step
Epoch 70/130
1750/1750 - 124s - loss: 24.3718 - mse: 14.7285 - val_loss: 86.1827 - val_mse: 56.2675 - 124s/epoch - 71ms/step
Epoch 71/130
1750/1750 - 124s - loss: 23.3191 - mse: 14.0792 - val_loss: 79.9859 - val_mse: 51.9644 - 124s/epoch - 71ms/step
Epoch 72/130
1750/1750 - 124s - loss: 22.1932 - mse: 13.3563 - val_loss: 95.6174 - val_mse: 63.5821 - 124s/epoch - 71ms/step
Epoch 73/130
1750/1750 - 126s - loss: 21.3528 - mse: 12.8401 - val_loss: 85.6448 - val_mse: 55.2907 - 126s/epoch - 72ms/step
Epoch 74/130
1750/1750 - 127s - loss: 20.3915 - mse: 12.2437 - val_loss: 72.5705 - val_mse: 46.9455 - 127s/epoch - 72ms/step
Epoch 75/130
1750/1750 - 127s - loss: 19.3562 - mse: 11.6140 - val_loss: 89.6059 - val_mse: 58.3147 - 127s/epoch - 73ms/step
Epoch 76/130
1750/1750 - 126s - loss: 18.8212 - mse: 11.2865 - val_loss: 81.2229 - val_mse: 51.8986 - 126s/epoch - 72ms/step
Epoch 77/130
1750/1750 - 126s - loss: 18.0358 - mse: 10.7945 - val_loss: 81.5893 - val_mse: 52.5560 - 126s/epoch - 72ms/step
Epoch 78/130
1750/1750 - 126s - loss: 17.6319 - mse: 10.5515 - val_loss: 80.0241 - val_mse: 51.2203 - 126s/epoch - 72ms/step
Epoch 79/130
1750/1750 - 126s - loss: 16.4609 - mse: 9.8516 - val_loss: 93.7812 - val_mse: 61.0731 - 126s/epoch - 72ms/step
Epoch 80/130
1750/1750 - 127s - loss: 15.8175 - mse: 9.4407 - val_loss: 67.2466 - val_mse: 43.5303 - 127s/epoch - 72ms/step
Epoch 81/130
1750/1750 - 127s - loss: 15.0945 - mse: 9.0156 - val_loss: 69.2725 - val_mse: 44.7399 - 127s/epoch - 72ms/step
Epoch 82/130
1750/1750 - 129s - loss: 14.7933 - mse: 8.8399 - val_loss: 105.0759 - val_mse: 69.6241 - 129s/epoch - 74ms/step
Epoch 83/130
1750/1750 - 129s - loss: 13.8728 - mse: 8.2683 - val_loss: 79.0939 - val_mse: 50.5981 - 129s/epoch - 74ms/step
Epoch 84/130
1750/1750 - 129s - loss: 13.2982 - mse: 7.9071 - val_loss: 93.8658 - val_mse: 60.9902 - 129s/epoch - 74ms/step
Epoch 85/130
1750/1750 - 129s - loss: 12.8781 - mse: 7.6568 - val_loss: 84.2658 - val_mse: 54.3186 - 129s/epoch - 74ms/step
Epoch 86/130
1750/1750 - 129s - loss: 12.3597 - mse: 7.3606 - val_loss: 80.3479 - val_mse: 51.6406 - 129s/epoch - 74ms/step
Epoch 87/130
1750/1750 - 125s - loss: 11.5915 - mse: 6.8884 - val_loss: 93.0484 - val_mse: 60.7809 - 125s/epoch - 71ms/step
Epoch 88/130
1750/1750 - 124s - loss: 11.1380 - mse: 6.6198 - val_loss: 77.4566 - val_mse: 50.1247 - 124s/epoch - 71ms/step
Epoch 89/130
1750/1750 - 123s - loss: 10.7289 - mse: 6.3746 - val_loss: 70.9562 - val_mse: 45.9266 - 123s/epoch - 70ms/step
Epoch 90/130
1750/1750 - 125s - loss: 10.2507 - mse: 6.0788 - val_loss: 79.2785 - val_mse: 50.7871 - 125s/epoch - 71ms/step
Epoch 91/130
1750/1750 - 123s - loss: 9.8638 - mse: 5.8563 - val_loss: 96.5082 - val_mse: 63.7988 - 123s/epoch - 71ms/step
Epoch 92/130
1750/1750 - 123s - loss: 9.3653 - mse: 5.5520 - val_loss: 66.1878 - val_mse: 42.6235 - 123s/epoch - 71ms/step
Epoch 93/130
1750/1750 - 124s - loss: 8.8708 - mse: 5.2671 - val_loss: 88.5899 - val_mse: 57.4057 - 124s/epoch - 71ms/step
Epoch 94/130
1750/1750 - 125s - loss: 8.4313 - mse: 5.0047 - val_loss: 72.7076 - val_mse: 47.0231 - 125s/epoch - 71ms/step
Epoch 95/130
1750/1750 - 124s - loss: 8.1410 - mse: 4.8294 - val_loss: 72.2777 - val_mse: 46.1589 - 124s/epoch - 71ms/step
Epoch 96/130
1750/1750 - 125s - loss: 7.7254 - mse: 4.5803 - val_loss: 69.2562 - val_mse: 44.2643 - 125s/epoch - 71ms/step
Epoch 97/130
1750/1750 - 126s - loss: 7.3522 - mse: 4.3563 - val_loss: 72.8385 - val_mse: 47.0051 - 126s/epoch - 72ms/step
Epoch 98/130
1750/1750 - 125s - loss: 7.0222 - mse: 4.1640 - val_loss: 94.0983 - val_mse: 61.2474 - 125s/epoch - 72ms/step
Epoch 99/130
1750/1750 - 125s - loss: 6.6705 - mse: 3.9639 - val_loss: 62.5124 - val_mse: 40.5947 - 125s/epoch - 71ms/step
Epoch 100/130
1750/1750 - 126s - loss: 6.3819 - mse: 3.7875 - val_loss: 62.2918 - val_mse: 40.3745 - 126s/epoch - 72ms/step
Epoch 101/130
1750/1750 - 125s - loss: 6.1176 - mse: 3.6316 - val_loss: 74.5072 - val_mse: 47.6416 - 125s/epoch - 71ms/step
Epoch 102/130
1750/1750 - 125s - loss: 5.8002 - mse: 3.4411 - val_loss: 87.2259 - val_mse: 56.2246 - 125s/epoch - 72ms/step
Epoch 103/130
1750/1750 - 124s - loss: 5.5373 - mse: 3.2911 - val_loss: 69.7303 - val_mse: 44.6189 - 124s/epoch - 71ms/step
Epoch 104/130
1750/1750 - 124s - loss: 5.2596 - mse: 3.1276 - val_loss: 80.6797 - val_mse: 51.3717 - 124s/epoch - 71ms/step
Epoch 105/130
1750/1750 - 124s - loss: 5.0239 - mse: 2.9925 - val_loss: 63.9201 - val_mse: 41.1327 - 124s/epoch - 71ms/step
Epoch 106/130
1750/1750 - 124s - loss: 4.8294 - mse: 2.8750 - val_loss: 72.6684 - val_mse: 46.2493 - 124s/epoch - 71ms/step
Epoch 107/130
1750/1750 - 123s - loss: 4.5648 - mse: 2.7231 - val_loss: 68.6238 - val_mse: 43.7865 - 123s/epoch - 70ms/step
Epoch 108/130
1750/1750 - 123s - loss: 4.3913 - mse: 2.6164 - val_loss: 64.6145 - val_mse: 41.5076 - 123s/epoch - 70ms/step
Epoch 109/130
1750/1750 - 124s - loss: 4.1498 - mse: 2.4771 - val_loss: 75.6580 - val_mse: 47.8983 - 124s/epoch - 71ms/step
Epoch 110/130
1750/1750 - 125s - loss: 4.0094 - mse: 2.3967 - val_loss: 59.6206 - val_mse: 38.5310 - 125s/epoch - 71ms/step
Epoch 111/130
1750/1750 - 126s - loss: 3.7931 - mse: 2.2710 - val_loss: 60.4841 - val_mse: 38.9444 - 126s/epoch - 72ms/step
Epoch 112/130
1750/1750 - 124s - loss: 3.6314 - mse: 2.1752 - val_loss: 63.4415 - val_mse: 40.5934 - 124s/epoch - 71ms/step
Epoch 113/130
1750/1750 - 125s - loss: 3.4932 - mse: 2.0934 - val_loss: 56.8774 - val_mse: 36.8211 - 125s/epoch - 71ms/step
Epoch 114/130
1750/1750 - 125s - loss: 3.3073 - mse: 1.9860 - val_loss: 74.7396 - val_mse: 47.4368 - 125s/epoch - 71ms/step
Epoch 115/130
1750/1750 - 125s - loss: 3.1837 - mse: 1.9173 - val_loss: 56.2421 - val_mse: 36.3998 - 125s/epoch - 71ms/step
Epoch 116/130
1750/1750 - 125s - loss: 3.0734 - mse: 1.8508 - val_loss: 54.4256 - val_mse: 35.3981 - 125s/epoch - 71ms/step
Epoch 117/130
1750/1750 - 125s - loss: 2.9721 - mse: 1.7910 - val_loss: 64.5592 - val_mse: 41.1439 - 125s/epoch - 71ms/step
Epoch 118/130
1750/1750 - 126s - loss: 2.8645 - mse: 1.7291 - val_loss: 51.8218 - val_mse: 33.7843 - 126s/epoch - 72ms/step
Epoch 119/130
1750/1750 - 126s - loss: 2.7767 - mse: 1.6754 - val_loss: 51.9889 - val_mse: 33.9978 - 126s/epoch - 72ms/step
Epoch 120/130
1750/1750 - 125s - loss: 2.7129 - mse: 1.6390 - val_loss: 50.7637 - val_mse: 33.2416 - 125s/epoch - 72ms/step
Epoch 121/130
1750/1750 - 125s - loss: 2.6579 - mse: 1.6063 - val_loss: 48.4178 - val_mse: 31.9284 - 125s/epoch - 71ms/step
Epoch 122/130
1750/1750 - 124s - loss: 2.5923 - mse: 1.5659 - val_loss: 50.2018 - val_mse: 32.9578 - 124s/epoch - 71ms/step
Epoch 123/130
1750/1750 - 124s - loss: 2.5356 - mse: 1.5335 - val_loss: 48.5627 - val_mse: 31.9337 - 124s/epoch - 71ms/step
Epoch 124/130
1750/1750 - 124s - loss: 2.4937 - mse: 1.5078 - val_loss: 47.9643 - val_mse: 31.6298 - 124s/epoch - 71ms/step
Epoch 125/130
1750/1750 - 126s - loss: 2.4457 - mse: 1.4770 - val_loss: 46.4029 - val_mse: 30.7003 - 126s/epoch - 72ms/step
Epoch 126/130
1750/1750 - 127s - loss: 2.4042 - mse: 1.4528 - val_loss: 47.5872 - val_mse: 31.4381 - 127s/epoch - 73ms/step
Epoch 127/130
1750/1750 - 128s - loss: 2.3821 - mse: 1.4414 - val_loss: 46.4947 - val_mse: 30.8297 - 128s/epoch - 73ms/step
Epoch 128/130
1750/1750 - 129s - loss: 2.3466 - mse: 1.4198 - val_loss: 46.1633 - val_mse: 30.6169 - 129s/epoch - 74ms/step
Epoch 129/130
1750/1750 - 130s - loss: 2.3158 - mse: 1.3999 - val_loss: 45.9448 - val_mse: 30.4846 - 130s/epoch - 74ms/step
Epoch 130/130
1750/1750 - 126s - loss: 2.2894 - mse: 1.3825 - val_loss: 45.9879 - val_mse: 30.5094 - 126s/epoch - 72ms/step
Traceback (most recent call last):
  File "incept_surface_model_op.py", line 277, in <module>
    main(args.epochs, args.data_path)
  File "incept_surface_model_op.py", line 258, in main
    print(f"Test Loss (MSE): {loss:.6f}")
TypeError: unsupported format string passed to list.__format__
