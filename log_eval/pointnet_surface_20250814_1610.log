nohup: ignoring input
2025-08-14 16:57:08.553563: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-14 16:57:08.571152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1755183428.591196 2743641 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1755183428.597312 2743641 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1755183428.613798 2743641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1755183428.613822 2743641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1755183428.613825 2743641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1755183428.613827 2743641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-08-14 16:57:08.618538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Torch: 2.7.1+cu126  CUDA available: True
GPUs: 2 ['NVIDIA RTX 6000 Ada Generation', 'NVIDIA RTX 6000 Ada Generation']
TensorFlow (for TFRecord I/O only): 2.19.0
Train shards: 80  Val shards: 20
I0000 00:00:1755183434.106678 2743641 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 45782 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:3d:00.0, compute capability: 8.9
I0000 00:00:1755183434.108259 2743641 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46550 MB memory:  -> device: 1, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:61:00.0, compute capability: 8.9
Total parameters: 3,134,892
Trainable parameters: 3,134,892
Detailed summary with torchinfo:
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
PointNet2SurfaceRegressor                [1, 10, 10, 3]            --
├─Sequential: 1-1                        [1, 64, 1225]             --
│    └─Conv1d: 2-1                       [1, 64, 1225]             192
│    └─BatchNorm1d: 2-2                  [1, 64, 1225]             128
│    └─ReLU: 2-3                         [1, 64, 1225]             --
├─PointNetSetAbstractionKNN: 1-2         [1, 3, 612]               --
│    └─ModuleList: 2-4                   --                        --
│    │    └─Sequential: 3-1              [1, 64, 612, 32]          4,416
│    │    └─Sequential: 3-2              [1, 64, 612, 32]          4,224
│    │    └─Sequential: 3-3              [1, 128, 612, 32]         8,448
├─PointNetSetAbstractionKNN: 1-3         [1, 3, 306]               --
│    └─ModuleList: 2-5                   --                        --
│    │    └─Sequential: 3-4              [1, 128, 306, 32]         17,024
│    │    └─Sequential: 3-5              [1, 128, 306, 32]         16,640
│    │    └─Sequential: 3-6              [1, 256, 306, 32]         33,280
├─PointNetSetAbstractionKNN: 1-4         [1, 3, 76]                --
│    └─ModuleList: 2-6                   --                        --
│    │    └─Sequential: 3-7              [1, 256, 76, 32]          66,816
│    │    └─Sequential: 3-8              [1, 256, 76, 32]          66,048
│    │    └─Sequential: 3-9              [1, 512, 76, 32]          132,096
├─PointNetSetAbstractionKNN: 1-5         [1, 3, 38]                --
│    └─ModuleList: 2-7                   --                        --
│    │    └─Sequential: 3-10             [1, 512, 38, 32]          264,704
│    │    └─Sequential: 3-11             [1, 512, 38, 32]          263,168
│    │    └─Sequential: 3-12             [1, 1024, 38, 32]         526,336
├─Sequential: 1-6                        [1, 512]                  --
│    └─Linear: 2-8                       [1, 1024]                 1,049,600
│    └─ReLU: 2-9                         [1, 1024]                 --
│    └─BatchNorm1d: 2-10                 [1, 1024]                 2,048
│    └─Dropout: 2-11                     [1, 1024]                 --
│    └─Linear: 2-12                      [1, 512]                  524,800
│    └─ReLU: 2-13                        [1, 512]                  --
│    └─BatchNorm1d: 2-14                 [1, 512]                  1,024
│    └─Dropout: 2-15                     [1, 512]                  --
├─Linear: 1-7                            [1, 300]                  153,900
==========================================================================================
Total params: 3,134,892
Trainable params: 3,134,892
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 2.89
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 241.41
Params size (MB): 12.54
Estimated Total Size (MB): 253.96
==========================================================================================
device count  2
start training for 100 epochs
2025-08-14 16:57:18.175633: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:381] TFRecordDataset `buffer_size` is unspecified, default to 262144
2025-08-14 16:57:28.129795: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] fused(ShuffleDatasetV3:3,RepeatDataset:4): Filling up shuffle buffer (this may take a while): 181945 of 250000
2025-08-14 16:57:31.999744: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:482] Shuffle buffer filled.
/home/ainsworth/master/train_pointnet_surface_torch.py:268: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  xyz = torch.from_numpy(xyz_np).to(_DEVICE).float()
  [train] step 500/2343  loss=417.8905  mse=334.2571
  [train] step 1000/2343  loss=407.2517  mse=324.5838
  [train] step 1500/2343  loss=402.8198  mse=320.5645
  [train] step 2000/2343  loss=399.8346  mse=317.8091
  [train] step 2343/2343  loss=398.3716  mse=316.4440
  [val] step 390/390  loss=346381.1265  mse=292828.4451
Epoch 001/100  train_loss=398.3716  train_mse=316.4440  val_loss=346381.1265  val_mse=292828.4451  time=1775.3s
  ↳ New best model saved to models/best_pointnet_surface_100.weights.h5 (val_loss=346381.1265)
  [train] step 500/2343  loss=386.7954  mse=305.9521
  [train] step 1000/2343  loss=384.9187  mse=304.1019
  [train] step 1500/2343  loss=382.4142  mse=301.7453
  [train] step 2000/2343  loss=380.2828  mse=299.7287
  [train] step 2343/2343  loss=378.1268  mse=297.7086
  [val] step 390/390  loss=566.8162  mse=463.8204
Epoch 002/100  train_loss=378.1268  train_mse=297.7086  val_loss=566.8162  val_mse=463.8204  time=1751.2s
  ↳ New best model saved to models/best_pointnet_surface_100.weights.h5 (val_loss=566.8162)
  [train] step 500/2343  loss=363.4009  mse=283.7083
  [train] step 1000/2343  loss=362.7557  mse=283.1531
  [train] step 1500/2343  loss=359.0621  mse=279.6872
  [train] step 2000/2343  loss=355.9605  mse=276.8280
  [train] step 2343/2343  loss=353.9907  mse=275.0174
  [val] step 390/390  loss=2526.3784  mse=1718.6463
Epoch 003/100  train_loss=353.9907  train_mse=275.0174  val_loss=2526.3784  val_mse=1718.6463  time=1760.4s
  [train] step 500/2343  loss=341.1733  mse=262.9617
  [train] step 1000/2343  loss=339.6141  mse=261.5470
  [train] step 1500/2343  loss=347.6327  mse=269.0051
